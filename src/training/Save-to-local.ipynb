{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "I am saving all pre-trained huggingface models, transformers and data-sets locally in a unified format for easier access later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models and Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fvd442/anaconda3/envs/noise-paper/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "BERT_MODEL = \"bert-base-cased\"\n",
    "\n",
    "path_model = f'./Models/Pretrained/{BERT_MODEL}.pt'\n",
    "path_tokenizer = f'./Tokenizers/Pretrained/{BERT_MODEL}.pt'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL,\n",
    "    do_lower_case = True ## Because BERT-uncased\n",
    "    )\n",
    "\n",
    "tokenizer.save_pretrained(path_tokenizer)\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    BERT_MODEL, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "model.save_pretrained(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "BASE_MODEL = \"roberta-base\"\n",
    "\n",
    "path_model = f'./Models/Pretrained/{BASE_MODEL}.pt'\n",
    "path_tokenizer = f'./Tokenizers/Pretrained/{BASE_MODEL}.pt'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL,\n",
    "    do_lower_case = True ## Because BERT-uncased\n",
    "    )\n",
    "\n",
    "tokenizer.save_pretrained(path_tokenizer)\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "model.save_pretrained(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import ElectraForSequenceClassification\n",
    "\n",
    "BASE_MODEL = \"google/electra-base-discriminator\"\n",
    "BASE_MODEL_OUT = \"electra-base\"\n",
    "\n",
    "path_model = f'./Models/Pretrained/{BASE_MODEL_OUT}.pt'\n",
    "path_tokenizer = f'./Tokenizers/Pretrained/{BASE_MODEL_OUT}.pt'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL,\n",
    "    do_lower_case = True ## Because BERT-uncased\n",
    "    )\n",
    "\n",
    "tokenizer.save_pretrained(path_tokenizer)\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = ElectraForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "model.save_pretrained(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "from transformers import GPT2Config\n",
    "\n",
    "\n",
    "BASE_MODEL = \"gpt2\"\n",
    "\n",
    "path_model = f'./Models/Pretrained/{BASE_MODEL}.pt'\n",
    "path_tokenizer = f'./Tokenizers/Pretrained/{BASE_MODEL}.pt'\n",
    "path_config = f'./Config/Pretrained/{BASE_MODEL}.pt'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL,\n",
    "    do_lower_case = False\n",
    "    )\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "\n",
    "# ## See info about adding tokens here: https://www.depends-on-the-definition.com/how-to-add-new-tokens-to-huggingface-transformers/\n",
    "# SPECIAL_TOKENS = {\n",
    "#     \"pad_token\": \"[PAD]\", # Does not originally come with padding token\n",
    "#     #\"additional_special_tokens\": [\"[SYS]\", \"[USR]\", \"[KG]\", \"[SUB]\", \"[PRED]\", \"[OBJ]\", \"[TRIPLE]\", \"[SEP]\", \"[Q]\",\"[DOM]\", 'frankie_and_bennys', 'cb17dy']\n",
    "# }\n",
    "# tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "\n",
    "# tokenizer.padding_side = 'left'\n",
    "\n",
    "# tokenizer.save_pretrained(path_tokenizer)\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "# add new, random embeddings for the new tokens\n",
    "model.resize_token_embeddings(len(tokenizer)) # must match vocabulary size\n",
    "model.config.pad_token_id = tokenizer.pad_token_id # must ensure model has pad_token\n",
    "\n",
    "# model.save_pretrained(path_model)\n",
    "\n",
    "\n",
    "# config = GPT2Config(BASE_MODEL, num_labels=2)\n",
    "# config.save_pretrained(path_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = \"roberta-base\"\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model2 = RobertaForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(BASE_MODEL,\n",
    "    do_lower_case = True ## Because BERT-uncased\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-350m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import OPTForSequenceClassification\n",
    "from transformers import OPTConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "BASE_MODEL = \"facebook/opt-350m\"\n",
    "BASE_MODEL_OUT = \"opt\"\n",
    "\n",
    "# path_model = f'./Models/Pretrained/{BASE_MODEL_OUT}.pt'\n",
    "# path_tokenizer = f'./Tokenizers/Pretrained/{BASE_MODEL_OUT}.pt'\n",
    "# path_config = f'./Config/Pretrained/{BASE_MODEL_OUT}.pt'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL,\n",
    "    do_lower_case = False\n",
    "    )\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "\n",
    "# ## See info about adding tokens here: https://www.depends-on-the-definition.com/how-to-add-new-tokens-to-huggingface-transformers/\n",
    "# SPECIAL_TOKENS = {\n",
    "#     \"pad_token\": \"[PAD]\", # Does not originally come with padding token\n",
    "#     #\"additional_special_tokens\": [\"[SYS]\", \"[USR]\", \"[KG]\", \"[SUB]\", \"[PRED]\", \"[OBJ]\", \"[TRIPLE]\", \"[SEP]\", \"[Q]\",\"[DOM]\", 'frankie_and_bennys', 'cb17dy']\n",
    "# }\n",
    "# tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "\n",
    "# tokenizer.padding_side = 'left'\n",
    "\n",
    "# tokenizer.save_pretrained(path_tokenizer)\n",
    "\n",
    "# model = OPTForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "\n",
    "model = OPTForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    # torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\" ### Speeds up inference\n",
    ")\n",
    "# # add new, random embeddings for the new tokens\n",
    "# model.resize_token_embeddings(len(tokenizer)) # must match vocabulary size\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id # must ensure model has pad_token\n",
    "\n",
    "# model.save_pretrained(path_model)\n",
    "\n",
    "\n",
    "# config = GPT2Config(BASE_MODEL, num_labels=2)\n",
    "# config.save_pretrained(path_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m      \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           OPTDecoder\n",
      "\u001b[0;31mString form:\u001b[0m   \n",
      "OPTDecoder(\n",
      "           (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
      "           (embed_positions): OPTLearne <...> rue)\n",
      "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "           )\n",
      "           )\n",
      "           )\n",
      "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/noise-paper-flashattn/lib/python3.9/site-packages/transformers/models/opt/modeling_opt.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`OPTDecoderLayer`]\n",
      "\n",
      "Args:\n",
      "    config: OPTConfig\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule."
     ]
    }
   ],
   "source": [
    "model.model.decoder?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemEval-2013 Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fvd442/anaconda3/envs/noise-paper/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    }
   ],
   "source": [
    "category_codes = {'positive':0, 'negative': 1} ## Ensure same mapping between all\n",
    "\n",
    "train = pd.read_csv(\"Data/SemEval/train/allTrainingData.tsv\", sep='\\t', header=None)\n",
    "train.columns = ['id', 'user', 'text_label', 'text']\n",
    "train.index.name='index'\n",
    "\n",
    "## Do we want neutral examples?\n",
    "## Currently proceeded with as if no\n",
    "\n",
    "train = train[train.text_label.isin(['positive', 'negative'])]\n",
    "train['label'] = train.text_label.map(category_codes)\n",
    "\n",
    "train = Dataset.from_pandas(train[['text','label']])\n",
    "\n",
    "dev = pd.read_csv(\"Data/SemEval/dev/twitdata_dev.tsv\", sep='\\t', header=None)\n",
    "dev.columns = ['id', 'user', 'text_label', 'text']\n",
    "dev.index.name='index'\n",
    "\n",
    "## Do we want neutral examples?\n",
    "## Currently proceeded with as if no\n",
    "\n",
    "dev = dev[dev.text_label.isin(['positive', 'negative'])]\n",
    "dev['label'] = dev.text_label.map(category_codes)\n",
    "\n",
    "dev = Dataset.from_pandas(dev[['text','label']])\n",
    "\n",
    "test = pd.read_csv(\"Data/SemEval/test/gold/twitdata_TEST.tsv\", sep='\\t', header=None, on_bad_lines='skip')\n",
    "test.columns = ['id', 'user', 'text_label', 'text']\n",
    "test.index.name='index'\n",
    "\n",
    "test = test[test.text_label.isin(['positive', 'negative'])]\n",
    "test['label'] = test.text_label.map(category_codes)\n",
    "\n",
    "test = Dataset.from_pandas(test[['text','label']])\n",
    "\n",
    "full_dataset = DatasetDict({\n",
    "    'train': train,\n",
    "    'test': test,\n",
    "    'validation': dev})\n",
    "\n",
    "path_dataset = \"./Data/SemEval\"\n",
    "\n",
    "full_dataset.cache_files\n",
    "\n",
    "full_dataset.save_to_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "\n",
    "category_codes = {'positive':0, 'negative': 1} ## Ensure same mapping between all\n",
    "\n",
    "lowlevel_anno = pd.read_csv(\"Data/SemEval/test/gold/twitter-test-gold-A.tsv\", sep='\\t', header=None, on_bad_lines='skip')\n",
    "# lowlevel_anno.columns = ['id', '_', 'start_span', 'end_span', 'span_annotation']\n",
    "test_text = pd.read_csv(\"Data/SemEval/test/gold/twitdata_TEST.tsv\", sep='\\t', header=None, on_bad_lines='skip')\n",
    "# test_text.columns = [['id', '_', 'label', 'text']]\n",
    "\n",
    "all_anno = lowlevel_anno[[0,2,3,4]].merge(test_text[[0,2,3]], how='inner', on=0)\n",
    "all_anno.columns = ['id', 'start_span', 'end_span', 'span_annotation', 'label', 'text']\n",
    "\n",
    "all_anno ['annotations'] = all_anno.text.map(lambda x: np.zeros(len(x.split())))\n",
    "all_anno['indices'] = all_anno.apply(lambda x: np.arange(x.start_span, x.end_span+1), axis=1)\n",
    "\n",
    "df = all_anno[['id', 'span_annotation', 'label', 'text', 'annotations', 'indices']].groupby(['id', 'label', 'text']).agg(({\n",
    "        'annotations': lambda x: x.tolist()[0],\n",
    "        'span_annotation': lambda x: x.tolist(),\n",
    "        'indices': lambda x: x.tolist()}\n",
    "                                           ))\n",
    "df = df.reset_index()\n",
    "\n",
    "def update_annos(annotations, span_labels, indices, text):\n",
    "    for i,span in enumerate(indices):\n",
    "        for j in span:\n",
    "            try:\n",
    "                if span_labels[i] == 'positive':\n",
    "                    annotations[j] = 1\n",
    "                elif span_labels[i] == 'negative':\n",
    "                    annotations[j] = -1\n",
    "            except IndexError:\n",
    "                # print(text)\n",
    "                return np.nan\n",
    "                \n",
    "    return annotations\n",
    "\n",
    "df['annotations'] = df.apply(lambda x: update_annos(x.annotations, x.span_annotation, x.indices, x.text), axis=1)\n",
    "\n",
    "df['label'] = df['label'].map(category_codes)\n",
    "df.index = df['id']\n",
    "df.index.name = 'index'\n",
    "out_df = df[['label', 'text', 'annotations']].dropna()\n",
    "test = Dataset.from_pandas(out_df[['label', 'text', 'annotations']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text', 'annotations', 'index'],\n",
       "    num_rows: 1659\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', 'annotations', 'index'],\n",
       "        num_rows: 1659\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset = DatasetDict({\n",
    "    'test': test,})\n",
    "\n",
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    }
   ],
   "source": [
    "path_dataset = \"./Data/Clean/SemEval\"\n",
    "\n",
    "full_dataset.cache_files\n",
    "\n",
    "full_dataset.save_to_disk(path_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fvd442/anaconda3/envs/noise-paper/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset sst2 (/home/fvd442/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n",
      "100%|██████████| 3/3 [00:00<00:00, 181.69it/s]\n",
      "                                                                                                 \r"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "path_dataset = \"./Data/SST-2\"\n",
    "\n",
    "dataset = load_dataset(\"sst2\")\n",
    "dataset.cache_files\n",
    "\n",
    "dataset.save_to_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fvd442/anaconda3/envs/noise-paper/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['idx', 'sentence', 'label'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "path_dataset = \"./Data/SST-2\"\n",
    "dataset = load_from_disk(path_dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Determine max length\n",
    "\n",
    "lens = [len(x.split()) for x in dataset['train']['sentence']]\n",
    "max(lens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get subset of Hummingbird in SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_from_disk, Dataset, DatasetDict\n",
    "\n",
    "\n",
    "DATA = \"SST-2\"\n",
    "\n",
    "### Load SST test data\n",
    "SST_dataset = load_from_disk(f\"./Data/{DATA}\")['validation'] #.map(tokenize_function, batched=True) #.map(reference_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': [0, 1, 2, 3, 4],\n",
       " 'sentence': [\"it 's a charming and often affecting journey . \",\n",
       "  'unflinchingly bleak and desperate ',\n",
       "  'allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . ',\n",
       "  \"the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . \",\n",
       "  \"it 's slow -- very , very slow . \"],\n",
       " 'label': [1, 0, 1, 1, 0]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SST_dataset[:5] ### Positive = 1, Negaitve = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_label</th>\n",
       "      <th>avg_label_score</th>\n",
       "      <th>orig_text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>perception_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>An artful , intelligent film that stays within...</td>\n",
       "      <td>an artful intelligent film that stays within t...</td>\n",
       "      <td>0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Utterly lacking in charm , wit and invention ,...</td>\n",
       "      <td>utterly lacking in charm wit and invention rob...</td>\n",
       "      <td>-0.3333 -1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Generally, I'd anticipate a code bug. Can you ...</td>\n",
       "      <td>generally i'd anticipate a code bug can you pa...</td>\n",
       "      <td>0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Thanks very much indeed! :-) I don't suppose y...</td>\n",
       "      <td>thanks very much indeed :-) i don't suppose yo...</td>\n",
       "      <td>0.6667 0.0 0.0 0.6667 0.0 0.0 0.0 0.0 0.0 0.66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Don't ever kiss the other girl in a 3some, don...</td>\n",
       "      <td>don't ever kiss the other girl in a 3some don'...</td>\n",
       "      <td>-0.3333 0.0 0.0 0.0 0.0 0.0 0.0 0.0 -0.3333 -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   human_label  avg_label_score  \\\n",
       "0          0.0              1.0   \n",
       "1          1.0              1.0   \n",
       "2          0.5              1.0   \n",
       "3          0.0              1.0   \n",
       "4          1.0              1.0   \n",
       "\n",
       "                                           orig_text  \\\n",
       "0  An artful , intelligent film that stays within...   \n",
       "1  Utterly lacking in charm , wit and invention ,...   \n",
       "2  Generally, I'd anticipate a code bug. Can you ...   \n",
       "3  Thanks very much indeed! :-) I don't suppose y...   \n",
       "4  Don't ever kiss the other girl in a 3some, don...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  an artful intelligent film that stays within t...   \n",
       "1  utterly lacking in charm wit and invention rob...   \n",
       "2  generally i'd anticipate a code bug can you pa...   \n",
       "3  thanks very much indeed :-) i don't suppose yo...   \n",
       "4  don't ever kiss the other girl in a 3some don'...   \n",
       "\n",
       "                                   perception_scores  \n",
       "0  0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0....  \n",
       "1  -0.3333 -1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0...  \n",
       "2  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0....  \n",
       "3  0.6667 0.0 0.0 0.6667 0.0 0.0 0.0 0.0 0.0 0.66...  \n",
       "4  -0.3333 0.0 0.0 0.0 0.0 0.0 0.0 0.0 -0.3333 -0...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hum_dataset = pd.read_csv(\"./Data/Hummingbird/sentiment.tsv\", sep='\\t')\n",
    "hum_dataset.head()\n",
    "\n",
    "# Positive = 0, Negative = 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "hum_sentences = hum_dataset.orig_text.map(lambda x: x.lower() + ' ').to_list()\n",
    "\n",
    "mask = []\n",
    "\n",
    "for i, h in enumerate(hum_sentences):\n",
    "    if h in SST_dataset['sentence']:\n",
    "        mask.append(True)\n",
    "    else:\n",
    "        mask.append(False)\n",
    "        \n",
    "print(sum(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1795237/2842011880.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  masked['sentence'] = masked['sentence'].map(lambda x: x.lower() + ' ') ## To match SST dataset\n",
      "/tmp/ipykernel_1795237/2842011880.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  masked['label'] = masked['label'].map(category_codes)\n",
      "/tmp/ipykernel_1795237/2842011880.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  masked.dropna(inplace=True) # Some neutral values that become NaN. Just drop them from the list.\n"
     ]
    }
   ],
   "source": [
    "masked = hum_dataset[mask]\n",
    "masked.index.name = \"index\"\n",
    "masked.columns = ['label', 'x', 'sentence', 'text', 'annotations'] # match labels to what I chose for SemEval\n",
    "masked['sentence'] = masked['sentence'].map(lambda x: x.lower() + ' ') ## To match SST dataset\n",
    "\n",
    "### Labels are reversed btwn SST and Hummingbird somehow? Return 1 \n",
    "category_codes = {1:0, 0: 1}\n",
    "masked['label'] = masked['label'].map(category_codes)\n",
    "masked.dropna(inplace=True) # Some neutral values that become NaN. Just drop them from the list.\n",
    "\n",
    "mask_final = Dataset.from_pandas(masked[['label', 'text', 'annotations']]) ### only use processed sentence as we need that for annotations\n",
    "path_dataset = \"./Data/Interim/Hummingbird\"\n",
    "\n",
    "# mask_final.cache_files\n",
    "# mask_final.save_to_disk(path_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', 'annotations', 'index'],\n",
       "        num_rows: 63\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict({\n",
    "    'test': mask_final})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    }
   ],
   "source": [
    "dataset.cache_files\n",
    "dataset.save_to_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HateXplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fvd442/anaconda3/envs/noise-paper/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset hatexplain (/home/fvd442/.cache/huggingface/datasets/hatexplain/plain_text/1.0.0/df474d8d8667d89ef30649bf66e9c856ad8305bef4bc147e8e31cbdf1b8e0249)\n",
      "100%|██████████| 3/3 [00:00<00:00, 80.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
       "        num_rows: 15383\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
       "        num_rows: 1922\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'annotators', 'rationales', 'post_tokens'],\n",
       "        num_rows: 1924\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from statistics import mode\n",
    "import numpy as np\n",
    "\n",
    "# 0 Offensive, 1 Normal, 2 Hatespeech\n",
    "dataset = load_dataset(\"hatexplain\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'text', 'label'],\n",
       "    num_rows: 1922\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.DataFrame(dataset[\"train\"])\n",
    "train[\"text\"] = train[\"post_tokens\"].map(lambda example: ' '.join(example))\n",
    "\n",
    "annos = pd.DataFrame.from_dict(dataset[\"train\"]['annotators'])\n",
    "annos[\"label\"] = annos[\"label\"].map(lambda x: mode(x))\n",
    "\n",
    "out = train[['text']].merge(annos[\"label\"], left_index=True, right_index=True).reset_index()\n",
    "\n",
    "train = Dataset.from_pandas(out)\n",
    "\n",
    "validation = pd.DataFrame(dataset[\"validation\"])\n",
    "validation[\"text\"] = validation[\"post_tokens\"].map(lambda example: ' '.join(example))\n",
    "\n",
    "annos = pd.DataFrame.from_dict(dataset[\"validation\"]['annotators'])\n",
    "annos[\"label\"] = annos[\"label\"].map(lambda x: mode(x))\n",
    "\n",
    "out = validation[['text']].merge(annos[\"label\"], left_index=True, right_index=True).reset_index()\n",
    "validation = Dataset.from_pandas(out)\n",
    "\n",
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"hatexplain\")[\"train\"]\n",
    "\n",
    "# max_l = 0\n",
    "\n",
    "# for p in dataset[\"post_tokens\"]:\n",
    "#     if len(p)> max_l:\n",
    "#         max_l = len(p)\n",
    "        \n",
    "# max_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'text', 'annotations', 'label'],\n",
       "    num_rows: 1924\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_annotations(r, p):\n",
    "    try:\n",
    "        if len(r) > 1:\n",
    "            return np.mean(r, axis=0)\n",
    "        else:\n",
    "            return  [0.0]*len(p)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "    \n",
    "    \n",
    "test = pd.DataFrame(dataset[\"test\"])\n",
    "test[\"text\"] = test[\"post_tokens\"].map(lambda example: ' '.join(example))\n",
    "test[\"annotations\"] = test.apply(lambda x: merge_annotations(x['rationales'], x['post_tokens']), axis=1)\n",
    "\n",
    "annos = pd.DataFrame.from_dict(dataset[\"test\"]['annotators'])\n",
    "annos[\"label\"] = annos[\"label\"].map(lambda x: mode(x))\n",
    "\n",
    "out = test[['text', 'annotations']].merge(annos[\"label\"], left_index=True, right_index=True).reset_index()\n",
    "\n",
    "test = Dataset.from_pandas(out)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    }
   ],
   "source": [
    "full_dataset = DatasetDict({\n",
    "    'train': train,\n",
    "    'test': test,\n",
    "    'validation': validation})\n",
    "\n",
    "path_dataset = \"./Data/hatexplain\"\n",
    "\n",
    "full_dataset.cache_files\n",
    "\n",
    "full_dataset.save_to_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# clean_out = out[out['label'] != 1]\n",
    "\n",
    "# test = Dataset.from_pandas(clean_out)\n",
    "# path_dataset = \"./Data/Clean/hatexplain\"\n",
    "\n",
    "# test.cache_files\n",
    "\n",
    "# test.save_to_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_out = out[out['label'] != 1]\n",
    "\n",
    "#### Corrupted at this datapoint\n",
    "clean_out.at[331,'text'] = 'i forgot all about my steak in the damn sink 🤦 ion even wanna cook the bitch now'\n",
    "clean_out.at[331,'annotations'] =  [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "\n",
    "clean_out.at[1828,'text'] = 'i get on twitter just talk shit about you cause u a scary ass hoe 😂 amp i want your head hoe 🤷 but im off this shit good night 😘 ❗'\n",
    "clean_out.at[1828,'annotations'] = [0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. , 0. , 0. , 0. ,\n",
    "       1. , 1. , 0. , 0. , 0.5, 0.5, 0.5, 0.5, 1. , 0. , 0. , 0. , 0. ,\n",
    "       0. , 0. , 0. , 0. , 0. , 0.]\n",
    "\n",
    "clean_out.at[932,'text'] = 'peter kopteros peter kopteros <number> hours ago i sure hope that nappy headed lowlife never gets to play professional football ever again unless it in the ghetto they grew up in lol'\n",
    "clean_out.at[932,'annotations'] =  clean_out.loc[932]['annotations'][:-1]\n",
    "\n",
    "clean_out.at[407,'text'] = 'am not trying to be racist some of them mmg rude especially arabs mabuk tu takyah cerita la dah mabuk blh pulak drive 🤦'\n",
    "clean_out.at[407,'annotations'] =  clean_out.loc[407]['annotations'][:-3]\n",
    "\n",
    "clean_out.at[304,'text'] = 'recordbreakerr recordbreakerrrrrrrrrrrrrr5 hours ago this was a racist killing against a defenseless white woman the shooter was a somalian muzzie mandingo rapefugee who only got hired because of the affirmative action'\n",
    "clean_out.at[304,'annotations'] =  clean_out.loc[304]['annotations'][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    }
   ],
   "source": [
    "test = Dataset.from_pandas(clean_out)\n",
    "test = test.map(remove_columns=['__index_level_0__'])\n",
    "\n",
    "path_dataset = \"./Data/Clean/hatexplain\"\n",
    "\n",
    "full_dataset = DatasetDict({\n",
    "    'test': test})\n",
    "\n",
    "full_dataset.cache_files\n",
    "\n",
    "full_dataset.save_to_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am not trying to be racist some of them mmg rude especially arabs mabuk tu takyah cerita la dah mabuk blh pulak drive 🤦 24\n",
      "am not trying to constitute racist some of them mmg unmannered especially arabs mabuk tu takyah cerita la dah mabuk blh pulak drive 🤦 24\n",
      "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] 24\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "test = load_from_disk('Data/Noise/hatexplain')\n",
    "for i, idx in enumerate(test['index']):\n",
    "    if idx == 407:\n",
    "        break\n",
    "    \n",
    "text_label = 'text'\n",
    "print(test[text_label][i], len(test[text_label][i].split()))\n",
    "text_label = 'random_synonym_25'\n",
    "print(test[text_label][i], len(test[text_label][i].split()))\n",
    "text_label = 'annotations'\n",
    "print(test[text_label][i], len(test[text_label][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 <user> ‍ daily reminder that nick the spic is a faggot who gets off on catboy porn pathetic and sad\n",
      "109 <user> lets the degenerate kike <user> ‍ fuck him in the ass and cries piteously during the torrid act but deep down he very much aroused at the idea of a filthy jew dominating him that a real radical agenda next week tune in for the threesome with paul nehlen\n",
      "395 <user> ‍ i agree the catholic church loves their muzzie infiltration at any cost they give and give and give to the sand niggers at the cost of white catholics shame on them\n"
     ]
    }
   ],
   "source": [
    "## Identify invalid characters\n",
    "\n",
    "clean_out = load_from_disk(\"Data/Clean/hatexplain/\")['test']\n",
    "for i,t in enumerate(clean_out['text']):\n",
    "    if '\\u200d' in t or '\\u200f' in t or \" 🏽\" in t or \" 🏾\" in t or \" ️\" in t:\n",
    "        print(i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9824"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_out = load_from_disk(\"Data/Clean/esnli/\")['test']\n",
    "len(clean_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[395, \"annotations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(clean_out)\n",
    "df.at[80, \"text\"] = \"<user> daily reminder that nick the spic is a faggot who gets off on catboy porn pathetic and sad\"\n",
    "df.at[80, \"annotations\"] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "df.at[109, \"text\"] = \"<user> lets the degenerate kike <user> fuck him in the ass and cries piteously during the torrid act but deep down he very much aroused at the idea of a filthy jew dominating him that a real radical agenda next week tune in for the threesome with paul nehlen\"\n",
    "df.at[109, \"annotations\"] = [0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "df.at[395, \"text\"] = \"<user> i agree the catholic church loves their muzzie infiltration at any cost they give and give and give to the sand niggers at the cost of white catholics shame on them\"\n",
    "df.at[395, \"annotations\"] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>141</td>\n",
       "      <td>&lt;user&gt; daily reminder that nick the spic is a ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.33333333...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>198</td>\n",
       "      <td>&lt;user&gt; lets the degenerate kike &lt;user&gt; fuck hi...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>688</td>\n",
       "      <td>&lt;user&gt; i agree the catholic church loves their...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                               text  \\\n",
       "80     141  <user> daily reminder that nick the spic is a ...   \n",
       "109    198  <user> lets the degenerate kike <user> fuck hi...   \n",
       "395    688  <user> i agree the catholic church loves their...   \n",
       "\n",
       "                                           annotations  label  \n",
       "80   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.33333333...      0  \n",
       "109  [0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, ...      0  \n",
       "395  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...      0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    if len(row[\"text\"].split()) != len(row[\"annotations\"]):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from statistics import mode\n",
    "import numpy as np\n",
    "\n",
    "test = Dataset.from_pandas(df.loc[[80,109,395]])\n",
    "test = test.map(remove_columns=['__index_level_0__'])\n",
    "\n",
    "path_dataset = \"./Data/Clean/new_hatexplain\"\n",
    "\n",
    "full_dataset = DatasetDict({\n",
    "    'test': test})\n",
    "\n",
    "full_dataset.cache_files\n",
    "\n",
    "full_dataset.save_to_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hatexplain_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3523792/1035090114.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_out['text'] = df_out['text'].str.replace(\"\\u200d \", \"\").str.replace(\"\\u200f \", \"\").str.replace(\" 🏽\", \"\").str.replace(\" 🏾\", \"\").str.replace(\" ️ \",\"\").str.replace(\" ️\", \"\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "test = load_from_disk(\"Data/hatexplain\")['test']\n",
    "\n",
    "df = pd.DataFrame(test)\n",
    "df.set_index(\"index\", inplace=True)\n",
    "df_out = df[df.label == 1]\n",
    "df_out['text'] = df_out['text'].str.replace(\"\\u200d \", \"\").str.replace(\"\\u200f \", \"\").str.replace(\" 🏽\", \"\").str.replace(\" 🏾\", \"\").str.replace(\" ️ \",\"\").str.replace(\" ️\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    }
   ],
   "source": [
    "# \\u200d # \\ufeff\n",
    "\n",
    "test = Dataset.from_pandas(df_out)\n",
    "\n",
    "path_dataset = \"./Data/Clean/hatexplain_neutral\"\n",
    "\n",
    "full_dataset = DatasetDict({\n",
    "    'test': test})\n",
    "\n",
    "full_dataset.cache_files\n",
    "\n",
    "full_dataset.save_to_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'breaking911 verified account <user> 2 4 h24 hours ago more flashback video michigan democrat rep john conyers allegedly reading playboy magazine on a packed airplane in <number> allegations of sexual harassment have surfaced tonight'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test = load_from_disk(\"./Data/Clean/hatexplain_neutral\")['test']\n",
    "\n",
    "# for i, idx in enumerate(test[\"index\"]):\n",
    "#     if idx == 959:\n",
    "#         break\n",
    "# test[\"text\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "      <th>label</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, annotations, label, index]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "test = load_from_disk(\"./Data/Clean/hatexplain_neutral\")['test']\n",
    "df = pd.DataFrame(test)\n",
    "df.loc[df['text'][df['text'].str.contains(\"\\uffef\")].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eSNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fvd442/anaconda3/envs/noise-paper/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/fvd442/project/noise-paper/Save-to-local.ipynb Cell 52\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhendrix/home/fvd442/project/noise-paper/Save-to-local.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhendrix/home/fvd442/project/noise-paper/Save-to-local.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# 0 entailment 1 neutral 2 contradiction\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhendrix/home/fvd442/project/noise-paper/Save-to-local.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mesnli\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhendrix/home/fvd442/project/noise-paper/Save-to-local.ipynb#Y102sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m dataset\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/datasets/load.py:1785\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   1781\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1782\u001b[0m )\n\u001b[1;32m   1784\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1785\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1786\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1787\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1788\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1789\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1790\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1791\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1792\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1793\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1794\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1795\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1796\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   1797\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1798\u001b[0m )\n\u001b[1;32m   1800\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1801\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/datasets/load.py:1514\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1512\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1513\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1514\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1515\u001b[0m     path,\n\u001b[1;32m   1516\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1517\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1518\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1519\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1520\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1521\u001b[0m )\n\u001b[1;32m   1523\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/datasets/load.py:1200\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   1199\u001b[0m \u001b[39mif\u001b[39;00m filename \u001b[39min\u001b[39;00m [sibling\u001b[39m.\u001b[39mrfilename \u001b[39mfor\u001b[39;00m sibling \u001b[39min\u001b[39;00m dataset_info\u001b[39m.\u001b[39msiblings]:\n\u001b[0;32m-> 1200\u001b[0m     \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[1;32m   1201\u001b[0m         path,\n\u001b[1;32m   1202\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1203\u001b[0m         download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1204\u001b[0m         download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1205\u001b[0m         dynamic_modules_path\u001b[39m=\u001b[39;49mdynamic_modules_path,\n\u001b[1;32m   1206\u001b[0m     )\u001b[39m.\u001b[39mget_module()\n\u001b[1;32m   1207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1208\u001b[0m     \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithoutScript(\n\u001b[1;32m   1209\u001b[0m         path,\n\u001b[1;32m   1210\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1214\u001b[0m         download_mode\u001b[39m=\u001b[39mdownload_mode,\n\u001b[1;32m   1215\u001b[0m     )\u001b[39m.\u001b[39mget_module()\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/datasets/load.py:878\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithScript.__init__\u001b[0;34m(self, name, revision, download_config, download_mode, dynamic_modules_path)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_mode \u001b[39m=\u001b[39m download_mode\n\u001b[1;32m    877\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdynamic_modules_path \u001b[39m=\u001b[39m dynamic_modules_path\n\u001b[0;32m--> 878\u001b[0m increase_load_count(name, resource_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/datasets/load.py:161\u001b[0m, in \u001b[0;36mincrease_load_count\u001b[0;34m(name, resource_type)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mHF_DATASETS_OFFLINE \u001b[39mand\u001b[39;00m config\u001b[39m.\u001b[39mHF_UPDATE_DOWNLOAD_COUNTS:\n\u001b[1;32m    160\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         head_hf_s3(name, filename\u001b[39m=\u001b[39;49mname \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.py\u001b[39;49m\u001b[39m\"\u001b[39;49m, dataset\u001b[39m=\u001b[39;49m(resource_type \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    162\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/datasets/utils/file_utils.py:96\u001b[0m, in \u001b[0;36mhead_hf_s3\u001b[0;34m(identifier, filename, use_cdn, dataset, max_retries)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhead_hf_s3\u001b[39m(\n\u001b[1;32m     94\u001b[0m     identifier: \u001b[39mstr\u001b[39m, filename: \u001b[39mstr\u001b[39m, use_cdn\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dataset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_retries\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     95\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[requests\u001b[39m.\u001b[39mResponse, \u001b[39mException\u001b[39;00m]:\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m http_head(\n\u001b[1;32m     97\u001b[0m         hf_bucket_url(identifier\u001b[39m=\u001b[39;49midentifier, filename\u001b[39m=\u001b[39;49mfilename, use_cdn\u001b[39m=\u001b[39;49muse_cdn, dataset\u001b[39m=\u001b[39;49mdataset),\n\u001b[1;32m     98\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m     99\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/datasets/utils/file_utils.py:412\u001b[0m, in \u001b[0;36mhttp_head\u001b[0;34m(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\u001b[0m\n\u001b[1;32m    410\u001b[0m headers \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(headers) \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    411\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m get_datasets_user_agent(user_agent\u001b[39m=\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 412\u001b[0m response \u001b[39m=\u001b[39m _request_with_retry(\n\u001b[1;32m    413\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    414\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    415\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    416\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    417\u001b[0m     cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[1;32m    418\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49mallow_redirects,\n\u001b[1;32m    419\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    420\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    421\u001b[0m )\n\u001b[1;32m    422\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/datasets/utils/file_utils.py:319\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[1;32m    317\u001b[0m tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(), url\u001b[39m=\u001b[39;49murl, timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    320\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[39mexcept\u001b[39;00m (requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectTimeout, requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectionError) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/requests/api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/requests/sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    524\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    525\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[1;32m    526\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[1;32m    527\u001b[0m }\n\u001b[1;32m    528\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 529\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    531\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/requests/sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    644\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 645\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    647\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    648\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/requests/adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 440\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    441\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    442\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    443\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    444\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    445\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    446\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    447\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    448\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    449\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    450\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    451\u001b[0m         )\n\u001b[1;32m    453\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m'\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/urllib3/connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    404\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    405\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/urllib3/connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1056\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1057\u001b[0m         (\n\u001b[1;32m   1058\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1064\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/urllib3/connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    364\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[1;32m    365\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39msocket_options\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket_options\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper/lib/python3.9/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m source_address:\n\u001b[1;32m     84\u001b[0m         sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m     sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     86\u001b[0m     \u001b[39mreturn\u001b[39;00m sock\n\u001b[1;32m     88\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39merror \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from statistics import mode\n",
    "import numpy as np\n",
    "\n",
    "# 0 entailment 1 neutral 2 contradiction\n",
    "dataset = load_dataset(\"esnli\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame(dataset[\"train\"])\n",
    "train.drop(columns=['explanation_1', 'explanation_2', 'explanation_3'], inplace=True)\n",
    "train.reset_index(inplace=True)\n",
    "train.columns = [\"index\", \"text_1\", \"text_2\", \"label\"]\n",
    "train = Dataset.from_pandas(train)\n",
    "\n",
    "validation = pd.DataFrame(dataset[\"validation\"])\n",
    "validation.drop(columns=['explanation_1', 'explanation_2', 'explanation_3'], inplace=True)\n",
    "validation.reset_index(inplace=True)\n",
    "validation.columns = [\"index\", \"text_1\", \"text_2\", \"label\"]\n",
    "validation = Dataset.from_pandas(validation)\n",
    "\n",
    "test = pd.DataFrame(dataset[\"test\"])\n",
    "test.drop(columns=['explanation_1', 'explanation_2', 'explanation_3'], inplace=True)\n",
    "test.reset_index(inplace=True)\n",
    "test.columns = [\"index\", \"text_1\", \"text_2\", \"label\"]\n",
    "test = Dataset.from_pandas(test)\n",
    "\n",
    "full_dataset = DatasetDict({\n",
    "    'train': train,\n",
    "    'test': test,\n",
    "    'validation': validation})\n",
    "\n",
    "path_dataset = \"./Data/esnli\"\n",
    "\n",
    "full_dataset.cache_files\n",
    "\n",
    "full_dataset.save_to_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_1 = 0\n",
    "for t in train[\"text_1\"]:\n",
    "    if len(t.split()) > max_1:\n",
    "        max_1 = len(t.split())\n",
    "\n",
    "max_2 = 0\n",
    "for t in train[\"text_2\"]:\n",
    "    if len(t.split()) > max_2:\n",
    "        max_2 = len(t.split())      \n",
    "\n",
    "max_1 + max_2\n",
    "### Make max 254, is_lower = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pairID', 'gold_label', 'Sentence1', 'Sentence2', 'Explanation_1',\n",
       "       'Sentence1_marked_1', 'Sentence2_marked_1', 'Sentence1_Highlighted_1',\n",
       "       'Sentence2_Highlighted_1', 'Explanation_2', 'Sentence1_marked_2',\n",
       "       'Sentence2_marked_2', 'Sentence1_Highlighted_2',\n",
       "       'Sentence2_Highlighted_2', 'Explanation_3', 'Sentence1_marked_3',\n",
       "       'Sentence2_marked_3', 'Sentence1_Highlighted_3',\n",
       "       'Sentence2_Highlighted_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_test = pd.read_csv( \"./Data/esnli/esnli_test.csv\")\n",
    "real_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2876236/2323404802.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_test['anno_1_1'] = clean_test['Sentence1_marked_1'].map(lambda x: np.array([ 1 if '*' in w else 0 for w in x.split()]))\n",
      "/tmp/ipykernel_2876236/2323404802.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_test['anno_2_1'] = clean_test['Sentence2_marked_1'].map(lambda x: np.array([ 1 if '*' in w else 0 for w in x.split()]))\n",
      "/tmp/ipykernel_2876236/2323404802.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_test['anno_1_2'] = clean_test['Sentence1_marked_2'].map(lambda x: np.array([ 1 if '*' in w else 0 for w in x.split()]))\n",
      "/tmp/ipykernel_2876236/2323404802.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_test['anno_2_2'] = clean_test['Sentence2_marked_2'].map(lambda x: np.array([ 1 if '*' in w else 0 for w in x.split()]))\n",
      "/tmp/ipykernel_2876236/2323404802.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_test['anno_1_3'] = clean_test['Sentence1_marked_3'].map(lambda x: np.array([ 1 if '*' in w else 0 for w in x.split()]))\n",
      "/tmp/ipykernel_2876236/2323404802.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_test['anno_2_3'] = clean_test['Sentence2_marked_3'].map(lambda x: np.array([ 1 if '*' in w else 0 for w in x.split()]))\n",
      "/tmp/ipykernel_2876236/2323404802.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_test.drop(columns=[c for c in real_test.columns if 'marked' in c], inplace=True)\n",
      "/tmp/ipykernel_2876236/2323404802.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_test[\"annotation_1\"] = np.array(clean_test[[\"anno_1_1\", \"anno_1_2\", \"anno_1_3\"]]).mean(axis=1)\n",
      "/tmp/ipykernel_2876236/2323404802.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_test[\"annotation_2\"] = np.array(clean_test[[\"anno_2_1\", \"anno_2_2\", \"anno_2_3\"]]).mean(axis=1)\n",
      "/tmp/ipykernel_2876236/2323404802.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_test.drop(columns=[c for c in clean_test.columns if 'anno_' in c], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>annotation_1</th>\n",
       "      <th>annotation_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>This church choir sings to the masses as they ...</td>\n",
       "      <td>The church has cracks in the ceiling.</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>This church choir sings to the masses as they ...</td>\n",
       "      <td>The church is filled with song.</td>\n",
       "      <td>[0.0, 0.3333333333333333, 0.6666666666666666, ...</td>\n",
       "      <td>[0.0, 0.3333333333333333, 0.0, 0.6666666666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>This church choir sings to the masses as they ...</td>\n",
       "      <td>A choir singing at a baseball game.</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333...</td>\n",
       "      <td>[0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A woman with a green headscarf, blue shirt and...</td>\n",
       "      <td>The woman is young.</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>A woman with a green headscarf, blue shirt and...</td>\n",
       "      <td>The woman is very happy.</td>\n",
       "      <td>[0.3333333333333333, 0.3333333333333333, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9819</th>\n",
       "      <td>9819</td>\n",
       "      <td>2</td>\n",
       "      <td>Two women are observing something together.</td>\n",
       "      <td>Two women are standing with their eyes closed.</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9820</th>\n",
       "      <td>9820</td>\n",
       "      <td>0</td>\n",
       "      <td>Two women are observing something together.</td>\n",
       "      <td>Two girls are looking at something.</td>\n",
       "      <td>[0.0, 0.3333333333333333, 0.0, 0.6666666666666...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9821</th>\n",
       "      <td>9821</td>\n",
       "      <td>2</td>\n",
       "      <td>A man in a black leather jacket and a book in ...</td>\n",
       "      <td>A man is flying a kite.</td>\n",
       "      <td>[0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9822</th>\n",
       "      <td>9822</td>\n",
       "      <td>0</td>\n",
       "      <td>A man in a black leather jacket and a book in ...</td>\n",
       "      <td>A man is speaking in a classroom.</td>\n",
       "      <td>[0.0, 0.3333333333333333, 0.0, 0.0, 0.33333333...</td>\n",
       "      <td>[0.0, 0.6666666666666666, 0.0, 0.6666666666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9823</th>\n",
       "      <td>9823</td>\n",
       "      <td>1</td>\n",
       "      <td>A man in a black leather jacket and a book in ...</td>\n",
       "      <td>A man is teaching science in a classroom.</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.3333333333333333, 0.3333333333333333, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9824 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  label                                             text_1  \\\n",
       "0         0      1  This church choir sings to the masses as they ...   \n",
       "1         1      0  This church choir sings to the masses as they ...   \n",
       "2         2      2  This church choir sings to the masses as they ...   \n",
       "3         3      1  A woman with a green headscarf, blue shirt and...   \n",
       "4         4      0  A woman with a green headscarf, blue shirt and...   \n",
       "...     ...    ...                                                ...   \n",
       "9819   9819      2        Two women are observing something together.   \n",
       "9820   9820      0        Two women are observing something together.   \n",
       "9821   9821      2  A man in a black leather jacket and a book in ...   \n",
       "9822   9822      0  A man in a black leather jacket and a book in ...   \n",
       "9823   9823      1  A man in a black leather jacket and a book in ...   \n",
       "\n",
       "                                              text_2  \\\n",
       "0              The church has cracks in the ceiling.   \n",
       "1                    The church is filled with song.   \n",
       "2                A choir singing at a baseball game.   \n",
       "3                                The woman is young.   \n",
       "4                           The woman is very happy.   \n",
       "...                                              ...   \n",
       "9819  Two women are standing with their eyes closed.   \n",
       "9820             Two girls are looking at something.   \n",
       "9821                         A man is flying a kite.   \n",
       "9822               A man is speaking in a classroom.   \n",
       "9823       A man is teaching science in a classroom.   \n",
       "\n",
       "                                           annotation_1  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.3333333333333333, 0.6666666666666666, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.3333333333333333, 0.3333333333333333, 0.0, ...   \n",
       "...                                                 ...   \n",
       "9819                     [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]   \n",
       "9820  [0.0, 0.3333333333333333, 0.0, 0.6666666666666...   \n",
       "9821  [0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9822  [0.0, 0.3333333333333333, 0.0, 0.0, 0.33333333...   \n",
       "9823  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                           annotation_2  \n",
       "0                   [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]  \n",
       "1     [0.0, 0.3333333333333333, 0.0, 0.6666666666666...  \n",
       "2     [0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, ...  \n",
       "3                                  [0.0, 0.0, 0.0, 1.0]  \n",
       "4                             [0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "...                                                 ...  \n",
       "9819           [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]  \n",
       "9820  [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.666...  \n",
       "9821      [0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0]  \n",
       "9822  [0.0, 0.6666666666666666, 0.0, 0.6666666666666...  \n",
       "9823  [0.0, 0.3333333333333333, 0.3333333333333333, ...  \n",
       "\n",
       "[9824 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_codes = {  'entailment': 0,\n",
    "                    'neutral': 1,\n",
    "                    'contradiction':2}\n",
    "\n",
    "real_test[\"label\"] = real_test[\"gold_label\"].map(category_codes)\n",
    "real_test.rename(columns={'Sentence1': 'text_1', 'Sentence2': \"text_2\"}, inplace=True)\n",
    "clean_test = real_test[[\"label\", \"text_1\", \"text_2\"] + [c for c in real_test.columns if 'marked' in c]]\n",
    "clean_test['anno_1_1'] = clean_test['Sentence1_marked_1'].map(lambda x: np.array([ 1 if '*' in w else 0 for w in x.split()]))\n",
    "clean_test['anno_2_1'] = clean_test['Sentence2_marked_1'].map(lambda x: np.array([ 1 if '*' in w else 0 for w in x.split()]))\n",
    "clean_test['anno_1_2'] = clean_test['Sentence1_marked_2'].map(lambda x: np.array([ 1 if '*' in w else 0 for w in x.split()]))\n",
    "clean_test['anno_2_2'] = clean_test['Sentence2_marked_2'].map(lambda x: np.array([ 1 if '*' in w else 0 for w in x.split()]))\n",
    "clean_test['anno_1_3'] = clean_test['Sentence1_marked_3'].map(lambda x: np.array([ 1 if '*' in w else 0 for w in x.split()]))\n",
    "clean_test['anno_2_3'] = clean_test['Sentence2_marked_3'].map(lambda x: np.array([ 1 if '*' in w else 0 for w in x.split()]))\n",
    "\n",
    "# clean_test['len_2'] = clean_test['text_2'].map(lambda x: len(x.split()))\n",
    "clean_test.drop(columns=[c for c in real_test.columns if 'marked' in c], inplace=True)\n",
    "clean_test[\"annotation_1\"] = np.array(clean_test[[\"anno_1_1\", \"anno_1_2\", \"anno_1_3\"]]).mean(axis=1)\n",
    "clean_test[\"annotation_2\"] = np.array(clean_test[[\"anno_2_1\", \"anno_2_2\", \"anno_2_3\"]]).mean(axis=1)\n",
    "clean_test.drop(columns=[c for c in clean_test.columns if 'anno_' in c], inplace=True)\n",
    "clean_test.reset_index(inplace=True)\n",
    "\n",
    "clean_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'label', 'text_1', 'text_2', 'annotation_1', 'annotation_2'],\n",
       "    num_rows: 9824\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out = Dataset.from_pandas(clean_test)\n",
    "test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    }
   ],
   "source": [
    "test_out = Dataset.from_pandas(clean_test)\n",
    "full_out = DatasetDict({\n",
    "    'test': test_out})\n",
    "path_dataset = \"./Data/Clean/esnli\"\n",
    "\n",
    "full_out.cache_files\n",
    "\n",
    "full_out.save_to_disk(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 459,\n",
       " 'label': 2,\n",
       " 'text_1': 'Two roadside workers with lime green safety jackets, white hard hats and gloves on with construction cones in the background',\n",
       " 'text_2': 'A woman chastises another.',\n",
       " 'annotation_1': [0.3333333333333333,\n",
       "  0.6666666666666666,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'annotation_2': [0.0, 0.6666666666666666, 0.3333333333333333, 0.0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in data['test']['index']:\n",
    "    if i == 459:\n",
    "        break\n",
    "data['test'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "tar (child): /content/aclImdb_v1.tar.gz: Cannot open: No such file or directory\n",
      "tar (child): Error is not recoverable: exiting now\n",
      "tar: Child returned status 2\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    }
   ],
   "source": [
    "# download the dataset\n",
    "!wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# unzip it\n",
    "!tar -zxf /content/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# unzip it\n",
    "!tar -zxf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noise-paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
