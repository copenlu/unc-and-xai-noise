{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save best model hyperparameters to a config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read successful\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open(\"config.yaml\", \"r\") as yamlfile:\n",
    "    model_configs = yaml.load(yamlfile, Loader=yaml.FullLoader)\n",
    "    print(\"Read successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################### SST-2 ###################################################################\n",
    "\n",
    "\"\"\"\n",
    "ELECTRA: https://arxiv.org/pdf/2003.10555v1.pdf Table 7\n",
    "\n",
    "Learning Rate\t3e-4 for Small, 1e-4 for Base, 5e-5 for Large\n",
    "Adam ϵ\t1e-6\n",
    "Adam β1\t0.9\n",
    "Adam β2\t0.999\n",
    "Layerwise LR decay\t0.8 for Base/Small, 0.9 for Large\n",
    "Learning rate decay\tLinear\n",
    "Warmup fraction\t0.1\n",
    "Attention Dropout\t0.1\n",
    "Dropout\t0.1\n",
    "Weight Decay\t0\n",
    "Batch Size\t32\n",
    "Train Epochs\t10 for RTE and STS, 2 for SQuAD, 3 for other tasks\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model_configs['electra'] = {'sst-2' : {\n",
    "                                        \"learning_rate\": 1e-4,  \n",
    "                                        \"batch_size\": 32, \n",
    "                                        \"architecture\": 'electra',\n",
    "                                        \"dataset\": 'sst-2',\n",
    "                                        \"epochs\": 3, \n",
    "                                        \"random_seed\": 42,\n",
    "                                        \"adam_eps\" : 1e-6, \n",
    "                                        \"adam_b1\" : 0.9,\n",
    "                                        \"adam_b2\" : 0.999,\n",
    "                                        \"llrd\" : 0.8,\n",
    "                                        \"decay_type\": 'linear',\n",
    "                                        \"warmup_frac\" : 0.1,\n",
    "                                        \"attn_dropout\" : 0.1,\n",
    "                                        \"dropout\" : 0.1,\n",
    "                                        \"weight_decay\" : 0.0\n",
    "                                        }\n",
    "}\n",
    "\n",
    "\n",
    "# https://huggingface.co/gchhablani/bert-base-cased-finetuned-sst2\n",
    "\"\"\"The following hyperparameters were used during training:\n",
    "- learning_rate: 2e-05\n",
    "- train_batch_size: 16\n",
    "- eval_batch_size: 8\n",
    "- seed: 42\n",
    "- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n",
    "- lr_scheduler_type: linear\n",
    "- num_epochs: 3.0\"\"\"\n",
    "\n",
    "model_configs['bert'] = {'sst-2' : {\n",
    "                        \"learning_rate\": 2e-5,  \n",
    "                        \"batch_size\": 16, \n",
    "                        \"architecture\": 'bert',\n",
    "                        \"dataset\": 'sst-2',\n",
    "                        \"epochs\": 3, \n",
    "                        \"random_seed\": 42,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'linear',\n",
    "                        \"warmup_frac\" : 0, # DEFAULT\n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.0 # DEFAULT\n",
    "}\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "No official recommendations for roberta-base for SST-2, therefore following what is seen on this model card\n",
    "https://huggingface.co/Bhumika/roberta-base-finetuned-sst2\n",
    "\"\"\"\n",
    "model_configs['roberta'] = {'sst-2' : {\n",
    "                        \"learning_rate\": 2e-5,  \n",
    "                        \"batch_size\": 16, \n",
    "                        \"architecture\": 'roberta',\n",
    "                        \"dataset\": 'sst-2',\n",
    "                        \"epochs\": 5, \n",
    "                        \"random_seed\": 42,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'linear',\n",
    "                        \"warmup_frac\" : 0, # DEFAULT ## Change to 0.06 https://arxiv.org/pdf/1907.11692v1.pdf\n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.0 # DEFAULT ## Change to 0.1 after article? https://arxiv.org/pdf/1907.11692v1.pdf\n",
    "}}\n",
    "\n",
    "\"\"\"\n",
    "https://arxiv.org/pdf/1912.10165.pdf :\n",
    "To train our model we follow a procedure largely based on the training procedures described in\n",
    "Radford et al. (2019) with a few differences. All training is performed with a maximum sequence\n",
    "length of 512 tokens. In the full dataset training setting we utilize a learning rate of 4 × 10−5\n",
    "and\n",
    "a batch size of 128. When training with a quarter of the dataset we then used a learning rate of\n",
    "3 × 10−5\n",
    "and a batch size of 32. Our learning rate has a warmup period over 1% of the total training\n",
    "iterations before decaying according to a single cycle cosine decay schedule over 10 epochs. We\n",
    "utilize an Adam optimizer (Kingma and Ba, 2014) with decoupled weight decay (Loshchilov and\n",
    "Hutter, 2019) λ = 0.01. All our models are trained efficiently on V100 GPUs by utilizing mixed\n",
    "precision training with dynamic loss scaling (Micikevicius et al., 2017). Additionally, we use global\n",
    "gradient norm clipping of 1.0 to improve the stability of training large models. Lastly, we utilize\n",
    "attention and hidden state dropout (Srivastava et al., 2014) values of 0.1.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "model_configs['gpt2-medium'] = {'sst-2' : {\n",
    "                        \"learning_rate\": 4e-5,  \n",
    "                        \"batch_size\": 128, \n",
    "                        \"architecture\": 'gpt2-medium',\n",
    "                        \"dataset\": 'sst-2',\n",
    "                        \"epochs\": 10, \n",
    "                        \"random_seed\": 42,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'cosine',\n",
    "                        \"warmup_frac\" : 0.01, \n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.1 \n",
    "}\n",
    "}\n",
    "\n",
    "# from https://huggingface.co/tianyisun/opt-350m-finetuned-sst2\n",
    "\n",
    "model_configs['opt'] = {'sst-2' : {\n",
    "                        \"learning_rate\": 2e-5,  \n",
    "                        \"batch_size\": 16, \n",
    "                        \"architecture\": 'opt',\n",
    "                        \"dataset\": 'sst-2',\n",
    "                        \"epochs\": 5, \n",
    "                        \"random_seed\": 42,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'linear',\n",
    "                        \"warmup_frac\" : 0.0, \n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0. \n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs['roberta'] = {'sst-2' : {\n",
    "                        \"learning_rate\": 2e-5,  \n",
    "                        \"batch_size\": 16, \n",
    "                        \"architecture\": 'roberta',\n",
    "                        \"dataset\": 'sst-2',\n",
    "                        \"epochs\": 5, \n",
    "                        \"random_seed\": 42,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'linear',\n",
    "                        \"warmup_frac\" : 0.06, # article https://arxiv.org/pdf/1907.11692v1.pdf\n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.1 # article? https://arxiv.org/pdf/1907.11692v1.pdf\n",
    "}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################# SemEval ###################################################################################################\n",
    "\n",
    "\"\"\"\"All values determined after raytuning on SemEval dataset with search space of: \n",
    "\n",
    "        \"learning_rate\": tune.loguniform(1e-6, 1e-4),\n",
    "        \"num_train_epochs\": tune.choice(range(1, 10)),\n",
    "        \"seed\": tune.choice(range(1, 41)),\n",
    "        \"per_device_train_batch_size\": tune.choice([4, 8, 16, 32, 64]),\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Slurm-212.out\n",
    "# 'learning_rate': 3.4889766548903635e-06, 'num_train_epochs': 5, 'seed': 24, 'per_device_train_batch_size': 8 \n",
    "\n",
    "model_configs['electra']['semeval'] = {\n",
    "                                        \"learning_rate\": 3e-6,  \n",
    "                                        \"batch_size\": 8, \n",
    "                                        \"architecture\": 'electra',\n",
    "                                        \"dataset\": 'semeval',\n",
    "                                        \"epochs\": 5, \n",
    "                                        \"random_seed\": 24,\n",
    "                                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                                        \"adam_b1\" : 0.9,# DEFAULT\n",
    "                                        \"adam_b2\" : 0.999,# DEFAULT\n",
    "                                        \"llrd\" : None,\n",
    "                                        \"decay_type\": 'linear',\n",
    "                                        \"warmup_frac\" : 0 , # DEFAULT\n",
    "                                        \"attn_dropout\" : 0.1, # DEFAULT\n",
    "                                        \"dropout\" : 0.1, # DEFAULT\n",
    "                                        \"weight_decay\" : 0.0 # DEFAULT\n",
    "                                        }\n",
    "\n",
    "\n",
    "\n",
    "# Slurm-210.out\n",
    "# 'learning_rate': 1.3258269776216493e-05, 'num_train_epochs': 3, 'seed': 37, 'per_device_train_batch_size': 16\n",
    "\n",
    "model_configs['bert']['semeval'] =  {\n",
    "                        \"learning_rate\": 1e-5,  \n",
    "                        \"batch_size\": 16, \n",
    "                        \"architecture\": 'bert',\n",
    "                        \"dataset\": 'semeval',\n",
    "                        \"epochs\": 3, \n",
    "                        \"random_seed\": 37,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'linear',\n",
    "                        \"warmup_frac\" : 0, # DEFAULT\n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.0 # DEFAULT\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#slurm-211.out\n",
    "# {'learning_rate': 1.3258269776216493e-05, 'num_train_epochs': 3, 'seed': 37, 'per_device_train_batch_size': 16\n",
    "model_configs['roberta']['semeval'] =  {\n",
    "                        \"learning_rate\": 1e-5,  \n",
    "                        \"batch_size\": 16, \n",
    "                        \"architecture\": 'roberta',\n",
    "                        \"dataset\": 'semeval',\n",
    "                        \"epochs\": 3, \n",
    "                        \"random_seed\": 37,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'linear',\n",
    "                        \"warmup_frac\" : 0, # DEFAULT\n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.0 # DEFAULT\n",
    "}\n",
    "\n",
    "\n",
    "model_configs['gpt2-medium']['semeval'] =  {\n",
    "                        \"learning_rate\": 8e-5,  \n",
    "                        \"batch_size\": 32, \n",
    "                        \"architecture\": 'gpt2-medium',\n",
    "                        \"dataset\": 'semeval',\n",
    "                        \"epochs\": 7, \n",
    "                        \"random_seed\": 42,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'cosine',\n",
    "                        \"warmup_frac\" : 0.01, \n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.1 \n",
    "}\n",
    "\n",
    "# Best trial config: {'learning_rate': 6.8759316495854425e-06, 'epochs': 1, 'batch_size': 32, 'data': 'SemEval'}\n",
    "\n",
    "model_configs['opt']['semeval'] =  {\n",
    "                        \"learning_rate\": 7e-6,  \n",
    "                        \"batch_size\": 32, \n",
    "                        \"architecture\": 'opt',\n",
    "                        \"dataset\": 'semeval',\n",
    "                        \"epochs\": 1, \n",
    "                        \"random_seed\": 42,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'cosine',\n",
    "                        \"warmup_frac\" : 0.01, \n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.1 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################# Hatexplain ###################################################################################################\n",
    "\n",
    "\"\"\"\"All values determined after raytuning 25 trials on hatexplain dataset with search space of: \n",
    "\n",
    "        \"learning_rate\": tune.loguniform(1e-6, 1e-4),\n",
    "        \"num_train_epochs\": tune.choice(range(1, 10)),\n",
    "        \"seed\": tune.choice(range(1, 41)),\n",
    "        \"per_device_train_batch_size\": tune.choice([4, 8, 16, 32, 64]),\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# BEST RUN CONFIGURATION:  {'learning_rate': 2.113705944064574e-05, 'num_train_epochs': 2, 'seed': 6, 'per_device_train_batch_size': 8}\n",
    "\n",
    "model_configs['electra']['hatexplain'] = {\n",
    "                                        \"learning_rate\": 2e-5,  \n",
    "                                        \"batch_size\": 8, \n",
    "                                        \"architecture\": 'electra',\n",
    "                                        \"dataset\": 'hatexplain',\n",
    "                                        \"epochs\": 2, \n",
    "                                        \"random_seed\": 6,\n",
    "                                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                                        \"adam_b1\" : 0.9,# DEFAULT\n",
    "                                        \"adam_b2\" : 0.999,# DEFAULT\n",
    "                                        \"llrd\" : None,\n",
    "                                        \"decay_type\": 'linear',\n",
    "                                        \"warmup_frac\" : 0 , # DEFAULT\n",
    "                                        \"attn_dropout\" : 0.1, # DEFAULT\n",
    "                                        \"dropout\" : 0.1, # DEFAULT\n",
    "                                        \"weight_decay\" : 0.0 # DEFAULT\n",
    "                                        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#BEST RUN CONFIGURATION:  {'learning_rate': 2.00344279275607e-05, 'num_train_epochs': 5, 'seed': 2, 'per_device_train_batch_size': 32}\n",
    "\n",
    "model_configs['bert']['hatexplain'] =  {\n",
    "                        \"learning_rate\": 2e-5,  \n",
    "                        \"batch_size\": 32, \n",
    "                        \"architecture\": 'bert',\n",
    "                        \"dataset\": 'hatexplain',\n",
    "                        \"epochs\": 5, \n",
    "                        \"random_seed\": 2,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'linear',\n",
    "                        \"warmup_frac\" : 0, # DEFAULT\n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.0 # DEFAULT\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# BEST RUN CONFIGURATION:  {'learning_rate': 5.899741796710488e-06, 'num_train_epochs': 5, 'seed': 2, 'per_device_train_batch_size': 32}\n",
    "model_configs['roberta']['hatexplain'] =  {\n",
    "                        \"learning_rate\": 6e-6,  \n",
    "                        \"batch_size\": 32, \n",
    "                        \"architecture\": 'roberta',\n",
    "                        \"dataset\": 'hatexplain',\n",
    "                        \"epochs\": 5, \n",
    "                        \"random_seed\": 2,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'linear',\n",
    "                        \"warmup_frac\" : 0, # DEFAULT\n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.0 # DEFAULT\n",
    "}\n",
    "\n",
    "\n",
    "model_configs['gpt2-medium']['hatexplain'] =  {\n",
    "                        \"learning_rate\": 5e-5,  \n",
    "                        \"batch_size\": 32, \n",
    "                        \"architecture\": 'gpt2-medium',\n",
    "                        \"dataset\": 'hatexplain',\n",
    "                        \"epochs\": 6, \n",
    "                        \"random_seed\": 42,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'cosine',\n",
    "                        \"warmup_frac\" : 0.01, \n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.1 \n",
    "}\n",
    "\n",
    "# Best trial config: {'learning_rate': 9.316976782029757e-06, 'epochs': 1, 'batch_size': 8, 'data': 'hatexplain'}\n",
    "# Best trial final validation accuracy: 0.6929460580912863\n",
    "\n",
    "model_configs['opt']['hatexplain'] =  {\n",
    "                        \"learning_rate\": 9e-6,  \n",
    "                        \"batch_size\": 8, \n",
    "                        \"architecture\": 'opt',\n",
    "                        \"dataset\": 'hatexplain',\n",
    "                        \"epochs\": 1, \n",
    "                        \"random_seed\": 42,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'cosine',\n",
    "                        \"warmup_frac\" : 0.01, \n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.1 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST RUN CONFIGURATION:  {'learning_rate': 9.780337016659403e-06, 'num_train_epochs': 3, 'seed': 35, 'per_device_train_batch_size': 8}\n",
    "\n",
    "model_configs['bert']['esnli'] =  {\n",
    "                        \"learning_rate\": 1e-5,  \n",
    "                        \"batch_size\": 8, \n",
    "                        \"architecture\": 'bert',\n",
    "                        \"dataset\": 'esnli',\n",
    "                        \"epochs\": 3, \n",
    "                        \"random_seed\": 35,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'linear',\n",
    "                        \"warmup_frac\" : 0, # DEFAULT\n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.0 # DEFAULT\n",
    "}\n",
    "\n",
    "# BEST RUN RESULT: 0.9157691526112579\n",
    "# BEST RUN CONFIGURATION:  {'learning_rate': 1.1208547084229366e-05, 'num_train_epochs': 3, 'seed': 28, 'per_device_train_batch_size': 32}\n",
    "\n",
    "model_configs['electra']['esnli'] =  {\n",
    "                        \"learning_rate\": 1e-5,  \n",
    "                        \"batch_size\": 32, \n",
    "                        \"architecture\": 'electra',\n",
    "                        \"dataset\": 'esnli',\n",
    "                        \"epochs\": 3, \n",
    "                        \"random_seed\": 28,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'linear',\n",
    "                        \"warmup_frac\" : 0, # DEFAULT\n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.0 # DEFAULT\n",
    "}\n",
    "\n",
    "# BEST RUN RESULT: 0.9185124974598659\n",
    "# BEST RUN CONFIGURATION:  {'learning_rate': 4.6496174473363295e-06, 'num_train_epochs': 4, 'seed': 24, 'per_device_train_batch_size': 16}\n",
    "\n",
    "model_configs['roberta']['esnli'] =  {\n",
    "                        \"learning_rate\": 5e-6,  \n",
    "                        \"batch_size\": 16, \n",
    "                        \"architecture\": 'roberta',\n",
    "                        \"dataset\": 'esnli',\n",
    "                        \"epochs\": 4, \n",
    "                        \"random_seed\": 24,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'linear',\n",
    "                        \"warmup_frac\" : 0, # DEFAULT\n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.0 # DEFAULT\n",
    "}\n",
    "\n",
    "# Best trial config: {'learning_rate': 6.11332959997169e-05, 'epochs': 3, 'batch_size': 32, 'data': 'esnli'}\n",
    "# Best trial final validation loss: 0.34371566041916995\n",
    "# Best trial final validation accuracy: 0.8742334054834056\n",
    "\n",
    "model_configs['gpt2-medium']['esnli'] =  {\n",
    "                        \"learning_rate\": 6e-5,  \n",
    "                        \"batch_size\": 32, \n",
    "                        \"architecture\": 'gpt2-medium',\n",
    "                        \"dataset\": 'esnli',\n",
    "                        \"epochs\": 3, \n",
    "                        \"random_seed\": 42,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'cosine',\n",
    "                        \"warmup_frac\" : 0.01, \n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.1 \n",
    "}\n",
    "\n",
    "#Current best trial: ea990_00003 with loss=0.27794001855458095 and params={'learning_rate': 2.0650169550742063e-06, 'epochs': 2, 'batch_size': 16, 'data': 'esnli'}\n",
    "\n",
    "model_configs['opt']['esnli'] =  {\n",
    "                        \"learning_rate\": 2e-6,  \n",
    "                        \"batch_size\": 16, \n",
    "                        \"architecture\": 'opt',\n",
    "                        \"dataset\": 'esnli',\n",
    "                        \"epochs\": 2, \n",
    "                        \"random_seed\": 42,\n",
    "                        \"adam_eps\" : 1e-8, # DEFAULT\n",
    "                        \"adam_b1\" : 0.9, # DEFAULT\n",
    "                        \"adam_b2\" : 0.999, # DEFAULT\n",
    "                        \"llrd\" : None,\n",
    "                        \"decay_type\": 'cosine',\n",
    "                        \"warmup_frac\" : 0.01, \n",
    "                        \"attn_dropout\" : 0.1, # DEFAULT \n",
    "                        \"dropout\" : 0.1, # DEFAULT\n",
    "                        \"weight_decay\" : 0.1 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sst-2', 'semeval', 'hatexplain', 'esnli'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_configs['opt'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"config.yaml\", 'w') as yamlfile:\n",
    "    data = yaml.dump(model_configs, yamlfile)\n",
    "    print(\"Write successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sst-2', 'semeval', 'hatexplain', 'esnli'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_configs['gpt2-medium'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noise-paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
