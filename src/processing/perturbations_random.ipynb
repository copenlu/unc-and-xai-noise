{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fvd442/anaconda3/envs/noise-paper/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random\n",
    "from itertools import chain, compress\n",
    "import numpy as np\n",
    "import string\n",
    "from datasets import Dataset, load_from_disk, DatasetDict\n",
    "import json\n",
    "from random import randint\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load PPDB synonyms\n",
    "with open('Resources/ppdb_synonyms.json') as json_file:\n",
    "    clean_ppdb_synonyms = json.load(json_file)\n",
    "    \n",
    "with open('Resources/ppdb_synonyms_xxxl.json') as json_file:\n",
    "    clean_ppdb_synonyms_XL = json.load(json_file)\n",
    "    \n",
    "with open('Resources/ppdb_synonyms_xxxl_nopostag.json') as json_file:\n",
    "    clean_ppdb_synonyms_XXL = json.load(json_file)\n",
    "    \n",
    "    \n",
    "from_path_dataset = \"./Data/Clean/SST-2\"\n",
    "\n",
    "\n",
    "dataset = load_from_disk(from_path_dataset)['test'] #['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "twitter_ids = set()\n",
    "for line in dataset['text']:\n",
    "    for word in line.split():\n",
    "        vocab.add(word.lower())\n",
    "        if word[0] == '@':\n",
    "            twitter_ids.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = None\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def insert_char(string, index, chartoadd):\n",
    "    return string[:index] + chartoadd + string[index:]\n",
    "\n",
    "def swap_char(string, index, chartoadd):\n",
    "    return string[:index] + chartoadd + string[index+1:]\n",
    "\n",
    "def charinsert(iterable):\n",
    "    return [insert_char(x, random.randint(0,len(x)), random.choice(string.ascii_letters)) for x in iterable]\n",
    "\n",
    "def random_charswap(iterable):\n",
    "    return [swap_char(x, random.randint(0,len(x)), random.choice(string.ascii_letters)) for x in iterable]\n",
    "\n",
    "def realistic_charswap(iterable):\n",
    "    return [butterfinger(x) for x in iterable]\n",
    "\n",
    "def butterfinger(text,errors=1,keyboard='querty'):\n",
    "    ### Adapted from https://github.com/alexyorke/butter-fingers/\n",
    "\n",
    "    keyApprox = {}\n",
    "\n",
    "    if keyboard == \"querty\": ## removed original word\n",
    "        keyApprox['q'] = \"wasedzx\"\n",
    "        keyApprox['w'] = \"qesadrfcx\"\n",
    "        keyApprox['e'] = \"wrsfdqazxcvgt\"\n",
    "        keyApprox['r'] = \"etdgfwsxcvgt\"\n",
    "        keyApprox['t'] = \"ryfhgedcvbnju\"\n",
    "        keyApprox['y'] = \"tugjhrfvbnji\"\n",
    "        keyApprox['u'] = \"yihkjtgbnmlo\"\n",
    "        keyApprox['i'] = \"uojlkyhnmlp\"\n",
    "        keyApprox['o'] = \"ipklujm\"\n",
    "        keyApprox['p'] = \"lo['ik\"\n",
    "\n",
    "        keyApprox['a'] = \"qszwxwdce\"\n",
    "        keyApprox['s'] = \"wxadrfv\"\n",
    "        keyApprox['d'] = \"ecsfaqgbv\"\n",
    "        keyApprox['f'] = \"dgrvwsxyhn\"\n",
    "        keyApprox['g'] = \"tbfhedcyjn\"\n",
    "        keyApprox['h'] = \"yngjfrvkim\"\n",
    "        keyApprox['j'] = \"hknugtblom\"\n",
    "        keyApprox['k'] = \"jlinyhn\"\n",
    "        keyApprox['l'] = \"okmpujn\"\n",
    "\n",
    "        keyApprox['z'] = \"axsvde\"\n",
    "        keyApprox['x'] = \"zcsdbvfrewq\"\n",
    "        keyApprox['c'] = \"xvdfzswergb\"\n",
    "        keyApprox['v'] = \"cfbgxdertyn\"\n",
    "        keyApprox['b'] = \"vnghcftyun\"\n",
    "        keyApprox['n'] = \"bmhjvgtuik\"\n",
    "        keyApprox['m'] = \"nkjloik\"\n",
    "        keyApprox[' '] = \" \"\n",
    "    else:\n",
    "        print(\"Keyboard not supported.\")\n",
    "  \n",
    "    if errors != 1:\n",
    "        print(\"Can only make one error per text\")\n",
    "        return text\n",
    "    \n",
    "    error_idx = randint(0,len(text)) ### Choose a random letter in the text\n",
    "    buttertext = \"\"\n",
    "    for i,letter in enumerate(text):\n",
    "        lcletter = letter.lower()\n",
    "        if not lcletter in keyApprox.keys():\n",
    "            newletter = lcletter\n",
    "        else:\n",
    "            if i == error_idx:\n",
    "                newletter = random.choice(keyApprox[lcletter])\n",
    "            else:\n",
    "                newletter = lcletter\n",
    "        # go back to original case\n",
    "        if not lcletter == letter:\n",
    "            newletter = newletter.upper()\n",
    "        buttertext += newletter\n",
    "\n",
    "    return buttertext\n",
    "\n",
    "\"\"\"\n",
    "Leet speak letter perturbation based on https://simple.wikipedia.org/wiki/Leet, excluding the space > 0.\n",
    "\"\"\"\n",
    "\n",
    "leet_letter_mappings = {\n",
    "    \"!\": \"1\",\n",
    "    \"A\": \"4\",\n",
    "    \"B\": \"8\",\n",
    "    \"E\": \"3\",\n",
    "    \"G\": \"6\",\n",
    "    \"I\": \"1\",\n",
    "    \"O\": \"0\",\n",
    "    \"S\": \"5\",\n",
    "    \"T\": \"7\",\n",
    "    \"X\": \"8\",\n",
    "    \"Z\": \"2\",\n",
    "    \"a\": \"@\",\n",
    "    \"b\": \"6\",\n",
    "    \"e\": \"3\",\n",
    "    \"g\": \"9\",\n",
    "    \"h\": \"4\",\n",
    "    \"i\": \"1\",\n",
    "    \"l\": \"1\",\n",
    "    \"o\": \"0\",\n",
    "    \"s\": \"5\",\n",
    "    \"t\": \"7\",\n",
    "    \"z\": \"2\"\n",
    "}\n",
    "\n",
    "def convert_to_leet(word):\n",
    "    global leet_letter_mappings\n",
    "    out = \"\"\n",
    "    for l in word:\n",
    "        if l in leet_letter_mappings.keys():\n",
    "            out += leet_letter_mappings[l]\n",
    "        else:\n",
    "            out += l\n",
    "    return out\n",
    "\n",
    "def insert_leet(iterable):\n",
    "    return [convert_to_leet(x) for x in iterable]\n",
    "\n",
    "\n",
    "# def wordswap(iterable):\n",
    "#     global pos_tags\n",
    "#     out = []\n",
    "#     for i,x in enumerate(iterable):\n",
    "#         try:\n",
    "#             out.append(random.choice ([ w.replace(\"_\", \"-\") for w in list(chain.from_iterable([word.lemma_names() for word in wn.synsets(x, pos=get_wordnet_pos(pos_tags[i]))])) if w != x]))  #pos=wn.VERB\n",
    "#             # First add more synsets here: openhownet? babelnet? article from pietro?\n",
    "#         except IndexError:\n",
    "#             # Get list of appropriate twitter aliases?\n",
    "#             # Get list of punctuation\n",
    "#             # Replace numbers?\n",
    "#             # Another determiner?\n",
    "#             out.append(x)\n",
    "#     return out\n",
    "\n",
    "def obscure_less(mask, to_remove):\n",
    "    old_masked = np.array(list(compress(range(len(mask)), mask)))\n",
    "\n",
    "    try:\n",
    "        removed = np.random.choice(old_masked, size=to_remove, replace=False)\n",
    "    except ValueError:\n",
    "        return old_masked\n",
    "        \n",
    "    if len(removed) > 0:\n",
    "        total_to_mask = np.setdiff1d(old_masked, removed)\n",
    "        \n",
    "        # Create new mask\n",
    "        new_mask = np.zeros(len(mask), dtype=int)\n",
    "        new_mask[total_to_mask] = 1\n",
    "        new_mask.astype(bool)\n",
    "    \n",
    "        return new_mask\n",
    "    else:\n",
    "        print(\"OOOPS\")\n",
    "        return old_masked\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import names\n",
    "\n",
    "NO_MATCH = set()\n",
    "no_match_counter = 0\n",
    "word_counter = 0\n",
    "weird_cases = set()\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "ones = {\n",
    "    0: '', 1: 'one', 2: 'two', 3: 'three', 4: 'four', 5: 'five', 6: 'six',\n",
    "    7: 'seven', 8: 'eight', 9: 'nine', 10: 'ten', 11: 'eleven', 12: 'twelve',\n",
    "    13: 'thirteen', 14: 'fourteen', 15: 'fifteen', 16: 'sixteen',\n",
    "    17: 'seventeen', 18: 'eighteen', 19: 'nineteen'}\n",
    "tens = {\n",
    "    2: 'twenty', 3: 'thirty', 4: 'forty', 5: 'fifty', 6: 'sixty',\n",
    "    7: 'seventy', 8: 'eighty', 9: 'ninety'}\n",
    "illions = {\n",
    "    1: 'thousand', 2: 'million', 3: 'billion', 4: 'trillion', 5: 'quadrillion',\n",
    "    6: 'quintillion', 7: 'sextillion', 8: 'septillion', 9: 'octillion',\n",
    "    10: 'nonillion', 11: 'decillion'}\n",
    "\n",
    "\n",
    "def say_number(i):\n",
    "    \"\"\"\n",
    "    Convert an integer in to it's word representation.\n",
    "\n",
    "    say_number(i: integer) -> string\n",
    "    \"\"\"\n",
    "    if i < 0:\n",
    "        return _join('negative', _say_number_pos(-i))\n",
    "    if i == 0:\n",
    "        return 'zero'\n",
    "    return _say_number_pos(i)\n",
    "\n",
    "\n",
    "def _say_number_pos(i):\n",
    "    if i < 20:\n",
    "        return ones[i]\n",
    "    if i < 100:\n",
    "        return _join(tens[i // 10], ones[i % 10])\n",
    "    if i < 1000:\n",
    "        return _divide(i, 100, 'hundred')\n",
    "    for illions_number, illions_name in illions.items():\n",
    "        if i < 1000**(illions_number + 1):\n",
    "            break\n",
    "    return _divide(i, 1000**illions_number, illions_name)\n",
    "\n",
    "\n",
    "def _divide(dividend, divisor, magnitude):\n",
    "    return _join(\n",
    "        _say_number_pos(dividend // divisor),\n",
    "        magnitude,\n",
    "        _say_number_pos(dividend % divisor),\n",
    "    )\n",
    "\n",
    "\n",
    "def _join(*args):\n",
    "    return '-'.join(filter(bool, args))\n",
    "\n",
    "def convert_to_case(old, new):\n",
    "    global weird_cases\n",
    "    if old.isupper():\n",
    "        return new.upper()\n",
    "    if old.islower():\n",
    "        return new.lower()\n",
    "    if old.istitle():\n",
    "        return new.title()\n",
    "    weird_cases.add(old)\n",
    "    return new\n",
    "\n",
    "def random_wordswap(iterable):\n",
    "    return [ find_random(x) for x in iterable]\n",
    "\n",
    "def find_random(word):\n",
    "    global vocab\n",
    "    return convert_to_case(word, random.choice([w for w in vocab if w != word.lower()]))\n",
    "\n",
    "def find_replacement(word, pos=''):\n",
    "    global twitter_ids\n",
    "    \n",
    "    # Get list of appropriate twitter aliases? and names?\n",
    "    # Get list of punctuation\n",
    "    quotes = [ \"'\", \"''\", \"`\", \"``\", '\"']\n",
    "    brackets = [\"(\", \")\", \"{\", \"}\", \"[\", \"]\", '/']\n",
    "    punct = [ '.', '!', '?', ',']\n",
    "    breaks = ['-', '--', ',', ':', ';']\n",
    "    \n",
    "    if word[:12] == \"http://t.co/\" : ##URL: \n",
    "        return  word[:-8] + ''.join(random.choice(string.ascii_letters + string.digits) for i in range(8))\n",
    "    \n",
    "    if word[:13] == \"https://t.co/\" : ##URL:\n",
    "        return  word[:-8] + ''.join(random.choice(string.ascii_letters + string.digits) for i in range(8))\n",
    "    \n",
    "    if word[0] == '#':\n",
    "        return('#' + find_replacement(word[1:]))\n",
    "    \n",
    "    if word[0] == '@': # twitter Id\n",
    "        return random.choice([t for t in twitter_ids if t != word])\n",
    "    \n",
    "    if pos == 'DT':\n",
    "        dets = ['a', 'an', 'the', 'this', 'that']\n",
    "        return convert_to_case(word, random.choice([d for d in dets if d != word.lower()]))\n",
    "    \n",
    "    if pos == 'WDT':\n",
    "        wdts = ['that', 'what', 'whatever', 'which', 'whichever']\n",
    "        return convert_to_case(word, random.choice([d for d in wdts if d != word.lower()]))\n",
    "    \n",
    "    # if pos == 'PRP$':\n",
    "    #     prps = ['her', 'his', 'mine', 'my', 'our', 'ours', 'their', 'your']\n",
    "    #     return random.choice([d for d in prps if d != word.lower()])\n",
    "    \n",
    "    # if pos == 'PRP':\n",
    "    #     prps = ['hers', 'herself', 'him', 'himself', 'hisself', 'it', 'itself', 'me', 'myself', 'one', 'oneself', 'ours', 'ourselves', 'ownself', 'she', 'theirs', 'them', 'themselves', 'they', 'us']\n",
    "        \n",
    "    if pos == 'NNP': # Proper noun\n",
    "        if word[-2:] == \"'s\":\n",
    "            return convert_to_case(word[:-2], random.choice([names.get_first_name(), names.get_last_name()])) + \"'s'\"\n",
    "        else:\n",
    "            return convert_to_case(word, random.choice([names.get_first_name(), names.get_last_name()]))\n",
    "    \n",
    "    \n",
    "    if word in quotes:\n",
    "        return random.choice([d for d in quotes if d != word.lower()])\n",
    "    \n",
    "    if word in brackets:\n",
    "        return random.choice([d for d in brackets if d != word.lower()])\n",
    "    \n",
    "    if word in punct:\n",
    "        return random.choice([d for d in punct if d != word.lower()])\n",
    "    \n",
    "    if word in breaks:\n",
    "        return random.choice([d for d in breaks if d != word.lower()])\n",
    "    \n",
    "    if word.isnumeric():\n",
    "        return say_number(int(word))\n",
    "        \n",
    "    # Collect wordnet synonyms\n",
    "    options_wn = [ w.replace(\"_\", \"-\") for w in list(chain.from_iterable([syn.lemma_names() for syn in wn.synsets(word.lower(), pos=get_wordnet_pos(pos))])) if w != word]\n",
    "    \n",
    "    if options_wn == []:\n",
    "        options_wn = [ w.replace(\"_\", \"-\") for w in list(chain.from_iterable([syn.lemma_names() for syn in wn.synsets(word.lower())])) if w != word]\n",
    "    \n",
    "    # Collect synonyms from PPDB\n",
    "    if pos != '':\n",
    "        try:\n",
    "            options_ppdb = clean_ppdb_synonyms[word.lower()][pos]\n",
    "        except KeyError:\n",
    "            options_ppdb = []\n",
    "    else:\n",
    "        try:\n",
    "            options_ppdb = clean_ppdb_synonyms_XXL[word.lower()]\n",
    "        except KeyError:\n",
    "            options_ppdb = []\n",
    "            \n",
    "    # Babelnet?? -- REQ PYTHON 3.8\n",
    "        \n",
    "    full_set = options_wn + options_ppdb\n",
    "    \n",
    "    try:\n",
    "        return convert_to_case(word,random.choice(full_set))\n",
    "        \n",
    "    except IndexError:\n",
    "        \n",
    "        if word[-1] in breaks:\n",
    "            return find_replacement(word[:-1]) + word[-1]\n",
    "            \n",
    "        if word[0] in breaks:\n",
    "            return word[0] + find_replacement(word[1:])\n",
    "        \n",
    "        if word[-1] in punct:\n",
    "            return find_replacement(word[:-1]) + word[-1]\n",
    "                    \n",
    "        if word[0] in punct:\n",
    "            return word[0] + find_replacement(word[1:])\n",
    "        \n",
    "        if word[-1] in quotes:\n",
    "            return find_replacement(word[:-1]) + word[-1]\n",
    "            \n",
    "        if word[0] in quotes:\n",
    "            return word[0] + find_replacement(word[1:])\n",
    "        \n",
    "        if word[-1] in brackets:\n",
    "            return find_replacement(word[:-1]) + word[-1]\n",
    "            \n",
    "        if word[0] in brackets:\n",
    "            return word[0] + find_replacement(word[1:])\n",
    "        \n",
    "        if word[-1] == \"%\":\n",
    "            return find_replacement(word[:-1]) + '%'\n",
    "        \n",
    "        ### Try to parse by hyphens\n",
    "        if '-' in word:\n",
    "            parts = word.split('-')\n",
    "            for i,p in enumerate(parts):\n",
    "                n = find_replacement(p)\n",
    "                if n != p:\n",
    "                    return '-'.join(parts[:i] + [n] + parts[i+1:])\n",
    "\n",
    "                \n",
    "        if '/' in word:\n",
    "            parts = word.split('/')\n",
    "            for i,p in enumerate(parts):\n",
    "                n = find_replacement(p)\n",
    "                if n != p:\n",
    "                    return '/'.join(parts[:i] + [n] + parts[i+1:])\n",
    "                \n",
    "        if '.' in word:\n",
    "            parts = word.split('.')\n",
    "            for i,p in enumerate(parts):\n",
    "                if p != '':\n",
    "                    n = find_replacement(p)\n",
    "                    if n != p:\n",
    "                        return '.'.join(parts[:i] + [n] + parts[i+1:])\n",
    "    \n",
    "\n",
    "        \n",
    "        # #### Try to parse TextLikeThis\n",
    "        # if len(word) > 1 and not word.isupper() and any(ele.isupper() for ele in word):\n",
    "        #     parts = re.findall('[a-zA-Z][^A-Z]*', word)\n",
    "        #     for i,p in enumerate(parts):\n",
    "        #         if p != '':\n",
    "        #             n = find_replacement(p)\n",
    "        #             if n != p:\n",
    "        #                 return ''.join(parts[:i] + [n] + parts[i+1:])\n",
    "        \n",
    "        ## check less good fits\n",
    "        try:\n",
    "            return convert_to_case(word,random.choice(clean_ppdb_synonyms_XL[word.lower()][pos]))\n",
    "        except (KeyError, IndexError):\n",
    "            try:\n",
    "                return convert_to_case(word,random.choice(clean_ppdb_synonyms_XXL[word.lower()]))\n",
    "            except ( KeyError, IndexError) :\n",
    "                if word[-3:] == 'ish':\n",
    "                    return find_replacement(word[:-3]) + 'ish'\n",
    "                if word[-4:] == 'ness':\n",
    "                    return find_replacement(word[:-4]) + 'ness'\n",
    "                if word[-4:] == 'less':\n",
    "                    return find_replacement(word[:-4]) + 'less'\n",
    "                # if word.istitle(): # implies proper noun\n",
    "                #     return random.choice([names.get_first_name(), names.get_last_name()])\n",
    "                return word\n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "def wordswap(iterable):\n",
    "    global pos_tags\n",
    "    global NO_MATCH\n",
    "    global no_match_counter\n",
    "    global word_counter\n",
    "    out = []\n",
    "    for i,x in enumerate(iterable):\n",
    "        word_counter += 1\n",
    "        new = find_replacement(x, pos_tags[i])\n",
    "        if new == x:\n",
    "            NO_MATCH.add((x, pos_tags[i]))\n",
    "            no_match_counter += 1\n",
    "        out.append(new)\n",
    "                \n",
    "    return out\n",
    "        \n",
    "        \n",
    "def create_random_masks(example):\n",
    "    masks = {}\n",
    "    noisy = {}\n",
    "\n",
    "    ### Start with everything\n",
    "    prop=1\n",
    "    mask = np.ones(len(example), dtype=int)\n",
    "    mask.astype(bool)\n",
    "    updated_lasttime= True\n",
    "\n",
    "    for additional in [.05]: #,.05,.1,.1,.2,.25]:\n",
    "        prop -= additional\n",
    "        if not updated_lasttime:\n",
    "            amt = old + additional\n",
    "        else:\n",
    "            amt = additional\n",
    "            \n",
    "        to_remove = round(amt * len(mask))\n",
    "        \n",
    "        if to_remove == 0:\n",
    "            updated_lasttime = False\n",
    "            old = additional\n",
    "        else:\n",
    "            updated_lasttime = True\n",
    "            mask = obscure_less(mask, to_remove)\n",
    "            \n",
    "        # masks[f'random_{prop*100:2.0f}'] = mask\n",
    "        # noisy[f'random_token_{prop*100:2.0f}'] = ' '.join(np.where(mask, '{TOKEN}' , example))    \n",
    "        # noisy[f'random_charswap_{prop*100:2.0f}'] = ' '.join(np.where(mask, charswap(example) , example))    \n",
    "        noisy[f'random_synonym_{prop*100:2.0f}'] = ' '.join(np.where(mask, wordswap(example) , example))    \n",
    "    \n",
    "    return noisy\n",
    "\n",
    "\n",
    "def match_pos_token_to_original(pos_tokens, raw_orig, pos_tags):\n",
    "    orig = []\n",
    "    for word in raw_orig:\n",
    "        if word != \"\":\n",
    "            orig.append(word.strip())\n",
    "    pos_idx = 0\n",
    "    last_pos_idx = 1\n",
    "    orig_idx = 0\n",
    "    \n",
    "    orig_to_pos_mapping = {}\n",
    "    orig_idx2token = {}\n",
    "\n",
    "    while pos_idx < len(pos_tokens) and orig_idx < len(orig):\n",
    "\n",
    "        current_orig = orig[orig_idx]\n",
    "        current_pos = pos_tokens[pos_idx]\n",
    "        orig_to_pos_mapping[orig_idx] = [pos_idx]\n",
    "        \n",
    "        pos_idx += 1\n",
    "        orig_idx2token[orig_idx] = current_orig\n",
    "        if current_pos != current_orig:\t\t\t\n",
    "            combined = current_pos\n",
    "            last_pos_idx = pos_idx\n",
    "            while last_pos_idx < len(pos_tokens):\n",
    "                next_part = pos_tokens[last_pos_idx]\t\t\t\t\n",
    "                combined += next_part\n",
    "                orig_to_pos_mapping[orig_idx].append(last_pos_idx)\n",
    "                if combined == current_orig:\t\t\t\t\t\n",
    "                    pos_idx = last_pos_idx + 1\n",
    "                    break\n",
    "                else:\n",
    "                    last_pos_idx += 1\n",
    "\n",
    "        orig_idx += 1\n",
    "        \n",
    "    pos_to_drop = [\"$\", '', \"(\", \")\", \",\", \"#\", \"POS\", \"--\", \".\", \":\", \"''\", '``']\n",
    "\n",
    "    new_pos_tags = []\n",
    "    for k in orig_to_pos_mapping.keys():\n",
    "        if len(orig_to_pos_mapping[k]) == 1:\n",
    "            new_pos_tags.append(pos_tags[orig_to_pos_mapping[k][0]])\n",
    "        else:\n",
    "            to_add = []\n",
    "            for i in orig_to_pos_mapping[k]:\n",
    "                if pos_tags[i] not in pos_to_drop:\n",
    "                    to_add.append(pos_tags[i])\n",
    "            if len(to_add) == 1:\n",
    "                new_pos_tags.append(to_add[0])\n",
    "            else:\n",
    "                new_pos_tags.append('')\n",
    "    \n",
    "    return new_pos_tags\n",
    "\n",
    "\n",
    "def insert_random_noise(example):\n",
    "    global pos_tags\n",
    "    global tokens\n",
    "    nltk_tokens = nltk.word_tokenize(example['text'])\n",
    "    pos_tags = [val[1] for val in nltk.pos_tag(nltk_tokens)]\n",
    "    \n",
    "    tokens = example['text'].split()\n",
    "    \n",
    "    pos_tags = match_pos_token_to_original(nltk_tokens, tokens, pos_tags)\n",
    "        \n",
    "    noise = create_random_masks(tokens)\n",
    "    return noise #example | noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_masks(example):\n",
    "    masks = {}\n",
    "    noisy = {}\n",
    "\n",
    "    ### Start with everything\n",
    "    prop=1\n",
    "    mask = np.ones(len(example), dtype=int)\n",
    "    mask.astype(bool)\n",
    "    updated_lasttime= True\n",
    "\n",
    "    for additional in [.05,.05,.1,.1,.2,.25]:\n",
    "        prop -= additional\n",
    "        if not updated_lasttime:\n",
    "            amt = old + additional\n",
    "        else:\n",
    "            amt = additional\n",
    "            \n",
    "        to_remove = round(amt * len(mask))\n",
    "        \n",
    "        if to_remove == 0:\n",
    "            updated_lasttime = False\n",
    "            old = additional\n",
    "        else:\n",
    "            updated_lasttime = True\n",
    "            mask = obscure_less(mask, to_remove)\n",
    "            \n",
    "        masks[f'random_{prop*100:2.0f}'] = mask\n",
    "        noisy[f'random_token_{prop*100:2.0f}'] = ' '.join(np.where(mask, '{TOKEN}' , example))    \n",
    "        noisy[f'random_charswap_{prop*100:2.0f}'] = ' '.join(np.where(mask, random_charswap(example) , example))    \n",
    "        noisy[f'random_synonym_{prop*100:2.0f}'] = ' '.join(np.where(mask, wordswap(example) , example))\n",
    "        noisy[f'random_butterfingers_{prop*100:2.0f}'] = ' '.join(np.where(mask, realistic_charswap(example) , example))\n",
    "        noisy[f'random_wordswap_{prop*100:2.0f}'] = ' '.join(np.where(mask, random_wordswap(example) , example))\n",
    "        noisy[f'random_charinsert_{prop*100:2.0f}'] = ' '.join(np.where(mask, charinsert(example) , example))\n",
    "        noisy[f'random_l33t_{prop*100:2.0f}'] = ' '.join(np.where(mask, insert_leet(example) , example))    \n",
    "    \n",
    "    return noisy\n",
    "\n",
    "\n",
    "# def insert_random_noise(example):\n",
    "#     global pos_tags\n",
    "#     nltk_tokens = nltk.word_tokenize(example['sentence'])\n",
    "#     pos_tags = [val[1] for val in nltk.pos_tag(nltk_tokens)]\n",
    "    \n",
    "#     tokens = example['sentence'].split()\n",
    "    \n",
    "#     noise = create_random_masks(tokens)\n",
    "#     return example | noise\n",
    "\n",
    "def insert_random_noise(example):\n",
    "    global pos_tags\n",
    "    nltk_tokens = nltk.word_tokenize(example['text'])\n",
    "    pos_tags = [val[1] for val in nltk.pos_tag(nltk_tokens)]\n",
    "    \n",
    "    tokens = example['text'].split()\n",
    "    \n",
    "    noise = create_random_masks(tokens)\n",
    "    return example | noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'it is an amazingly good movie. Watch it tonight!!!!',\n",
       " 'random_token_95': '{TOKEN} {TOKEN} {TOKEN} {TOKEN} {TOKEN} {TOKEN} {TOKEN} {TOKEN} {TOKEN}',\n",
       " 'random_charswap_95': 'kt Ts anG ahazingly geod moFie. Waxch itC toVight!!!!',\n",
       " 'random_synonym_95': 'information-technology be a astonishingly honorable flick. Determine information-technology wednesdays!!!!',\n",
       " 'random_butterfingers_95': 'it ms an amazingly gomd movie. Wanch ic tonight!!!!',\n",
       " 'random_wordswap_95': 'their touches beguiling get whose stretches Community art ski',\n",
       " 'random_charinsert_95': 'itG Ris van amahzingly golod moevie. Watich iet tonight!!!Q!',\n",
       " 'random_l33t_95': '17 15 @n @m@21n91y 900d m0v13. W@7c4 17 70n19471111',\n",
       " 'random_token_90': '{TOKEN} {TOKEN} an {TOKEN} {TOKEN} {TOKEN} {TOKEN} {TOKEN} {TOKEN}',\n",
       " 'random_charswap_90': 'Qt Ks an amazinHly goMd movin. Wabch iW tonightg!!!',\n",
       " 'random_synonym_90': 'it make-up an stunningly proficient moving-picture-show. Determine information-technology wednesdays!!!!',\n",
       " 'random_butterfingers_90': 'it os an amaziugly goob movie. Wacch ir tonjght!!!!',\n",
       " 'random_wordswap_90': 'transcends some an trash 93 fat Plot his settings',\n",
       " 'random_charinsert_90': 'itY Mis an amazinqgly goodt movieo. uWatch iit teonight!!!!',\n",
       " 'random_l33t_90': '17 15 an @m@21n91y 900d m0v13. W@7c4 17 70n19471111',\n",
       " 'random_token_80': 'it {TOKEN} an {TOKEN} {TOKEN} {TOKEN} {TOKEN} {TOKEN} {TOKEN}',\n",
       " 'random_charswap_80': 'it ia an amazingVy gGod movieU Wjtch Pt tonight!!!!d',\n",
       " 'random_synonym_80': 'it personify an surprisingly valid motion-picture-show. Learn information-technology night!!!!',\n",
       " 'random_butterfingers_80': 'it is an aiazingly gold kovie. Watci ut tonidht!!!!',\n",
       " 'random_wordswap_80': 'it adventure an egg time finally Trash for green',\n",
       " 'random_charinsert_80': 'it Tis an Oamazingly gojod mKovie. WNatch Lit tonight!!o!!',\n",
       " 'random_l33t_80': 'it 15 an @m@21n91y 900d m0v13. W@7c4 17 70n19471111',\n",
       " 'random_token_70': 'it is an {TOKEN} {TOKEN} {TOKEN} {TOKEN} {TOKEN} {TOKEN}',\n",
       " 'random_charswap_70': 'it is an akazingly gGod movieI Watqh if tonight!!!!j',\n",
       " 'random_synonym_70': 'it is an astonishingly undecomposed picture-show. Sentinel it this-evening!!!!',\n",
       " 'random_butterfingers_70': 'it is an aiazingly jood movme. Wateh it tunight!!!!',\n",
       " 'random_wordswap_70': 'it is an the get artfully Realize implodes serious',\n",
       " 'random_charinsert_70': 'it is an amazinHgly gooRd movipe. Wpatch iti tHonight!!!!',\n",
       " 'random_l33t_70': 'it is an @m@21n91y 900d m0v13. W@7c4 17 70n19471111',\n",
       " 'random_token_50': 'it is an {TOKEN} good {TOKEN} {TOKEN} it {TOKEN}',\n",
       " 'random_charswap_50': 'it is an amazinKly good moviem Watchc it Wonight!!!!',\n",
       " 'random_synonym_50': 'it is an marvellously good motion-picture-show. Watch it night!!!!',\n",
       " 'random_butterfingers_50': 'it is an amazingli good movie. Fatch it tonught!!!!',\n",
       " 'random_wordswap_50': 'it is an sad good escapist Endurance it adventure',\n",
       " 'random_charinsert_50': 'it is an amabzingly good moFvie. WatTch it tonighXt!!!!',\n",
       " 'random_l33t_50': 'it is an @m@21n91y good m0v13. W@7c4 it 70n19471111',\n",
       " 'random_token_25': 'it is an {TOKEN} good movie. Watch it {TOKEN}',\n",
       " 'random_charswap_25': 'it is an aEazingly good movie. Watch it bonight!!!!',\n",
       " 'random_synonym_25': 'it is an astonishingly good movie. Watch it this-evening!!!!',\n",
       " 'random_butterfingers_25': 'it is an amavingly good movie. Watch it tlnight!!!!',\n",
       " 'random_wordswap_25': 'it is an ratcatcher good movie. Watch it spanish',\n",
       " 'random_charinsert_25': 'it is an amaVzingly good movie. Watch it tonight!!!!S',\n",
       " 'random_l33t_25': 'it is an @m@21n91y good movie. Watch it 70n19471111'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = { 'text': 'it is an amazingly good movie. Watch it tonight!!!!'}\n",
    "\n",
    "insert_random_noise(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function insert_random_noise at 0x7f5db8d8ad30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map:   0%|          | 0/63 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'LazyRow' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/fvd442/project/noise-paper/perturbations_random.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhendrix/home/fvd442/project/noise-paper/perturbations_random.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m test \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(insert_random_noise)\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper_py3-8/lib/python3.8/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper_py3-8/lib/python3.8/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper_py3-8/lib/python3.8/site-packages/datasets/arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3091\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3092\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3093\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3094\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   3095\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3096\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3097\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3098\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3099\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper_py3-8/lib/python3.8/site-packages/datasets/arrow_dataset.py:3450\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3448\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3449\u001b[0m \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3450\u001b[0m     example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[1;32m   3451\u001b[0m     \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   3452\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper_py3-8/lib/python3.8/site-packages/datasets/arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3351\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3352\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3353\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3354\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3355\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3356\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3357\u001b[0m     }\n",
      "\u001b[1;32m/home/fvd442/project/noise-paper/perturbations_random.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhendrix/home/fvd442/project/noise-paper/perturbations_random.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m tokens \u001b[39m=\u001b[39m example[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msplit()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhendrix/home/fvd442/project/noise-paper/perturbations_random.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m noise \u001b[39m=\u001b[39m create_random_masks(tokens)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhendrix/home/fvd442/project/noise-paper/perturbations_random.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mreturn\u001b[39;00m example \u001b[39m|\u001b[39;49m noise\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'LazyRow' and 'dict'"
     ]
    }
   ],
   "source": [
    "test = dataset.map(insert_random_noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_path_dataset = \"./Data/Noise/SST-2/\"\n",
    "\n",
    "# dataset.cache_files\n",
    "\n",
    "# full_dataset = DatasetDict({\n",
    "#     'test': test,})\n",
    "\n",
    "# dataset.save_to_disk(out_path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m b\u001b[39m.\u001b[39mid\n",
      "\u001b[0;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "b.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Metaphor_(designers)', 'metaphor'}\n"
     ]
    }
   ],
   "source": [
    "import babelnet as bn\n",
    "from babelnet.language import Language\n",
    "from babelnet.pos import POS\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return POS.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return POS.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return POS.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return POS.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "byl = bn.get_synsets('metaphor', from_langs=[Language.EN], poses=[POS.NOUN])\n",
    "senses = set()\n",
    "for b in byl:\n",
    "    by = bn.get_synset(bn.BabelSynsetID(str(b.id)))\n",
    "    senses.add(by.main_sense(Language.EN).full_lemma)\n",
    "    for edge in by.outgoing_edges(): #bn.data.relation.BabelPointer.SIMILAR_TO\n",
    "        # print(edge.pointer)\n",
    "        if 'similar' in str(edge.pointer):\n",
    "            senses.add(bn.get_synset(edge.id_target).main_sense(Language.EN).full_lemma)\n",
    "            \n",
    "print(senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The arguments have to be homogeneous.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m by \u001b[39m=\u001b[39m bn\u001b[39m.\u001b[39mget_senses(bn\u001b[39m.\u001b[39mBabelSynsetID(\u001b[39mstr\u001b[39m(b\u001b[39m.\u001b[39mid)))\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m by:\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mprint\u001b[39m(bn\u001b[39m.\u001b[39;49mget_synset(s\u001b[39m.\u001b[39;49mid))\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(by)\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper_py3-8/lib/python3.8/site-packages/babelnet/apis/abstract_api.py:212\u001b[0m, in \u001b[0;36mAbstractAPI.get_synset\u001b[0;34m(self, resource_id, to_langs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_synset\u001b[39m(\u001b[39mself\u001b[39m, resource_id: ResourceID, to_langs: Optional[Set[Language]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[BabelSynset]:\n\u001b[1;32m    187\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the synset identified by the ResourceID in input.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[39m    Some examples that can be used follow::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m    @rtype: Optional[BabelSynset]\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     syns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_synsets(resource_id, to_langs\u001b[39m=\u001b[39;49mto_langs \u001b[39mif\u001b[39;49;00m to_langs \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    213\u001b[0m     \u001b[39mreturn\u001b[39;00m syns[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m syns \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper_py3-8/lib/python3.8/site-packages/babelnet/apis/abstract_api.py:184\u001b[0m, in \u001b[0;36mAbstractAPI.get_synsets\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_synsets(resource_ids\u001b[39m=\u001b[39mresource_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe arguments have to be homogeneous.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The arguments have to be homogeneous."
     ]
    }
   ],
   "source": [
    "byl = bn.get_synsets('metaphor', from_langs=[Language.EN], poses=[POS.NOUN])\n",
    "senses = set()\n",
    "for b in byl:\n",
    "    by = bn.get_senses(bn.BabelSynsetID(str(b.id)))\n",
    "    for s in by:\n",
    "        print(bn.get_synset(s.id))\n",
    "    print(by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Metaphor_(designers)', 'metaphor'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byl = bn.get_synsets('metaphor', from_langs=[Language.EN], poses=[POS.NOUN])\n",
    "senses = set()\n",
    "for b in byl:\n",
    "    by = bn.get_synset(bn.BabelSynsetID(str(b.id)))\n",
    "    senses.add(by.main_sense(Language.EN).full_lemma)\n",
    "    if 'hyponym' in str(edge.pointer) or 'hypernym' in str(edge.pointer):\n",
    "        senses.add(bn.get_synset(edge.id_target).main_sense(Language.EN).full_lemma)\n",
    "        \n",
    "senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar moving\n",
      "derivation pathos\n",
      "similar_to moving\n",
      "derivationally_related_form pathos\n",
      "gloss_related_form_(monosemous) affect\n",
      "gloss_related_form_(disambiguated) affect\n"
     ]
    }
   ],
   "source": [
    "test = bn.get_synsets('poignant', from_langs=[Language.EN], poses=[POS.ADJ])[0]\n",
    "for edge in test.outgoing_edges():\n",
    "    print(edge.pointer, bn.get_synset(edge.id_target).main_sense(Language.EN).full_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn:00108734a\tpoignant - similar_to - bn:00108168a - painful\n"
     ]
    }
   ],
   "source": [
    "by = bn.get_synset(bn.BabelSynsetID('bn:00108734a'))\n",
    "for edge in by.outgoing_edges(bn.data.relation.BabelPointer.SIMILAR_TO):\n",
    "    print(str(by.id) + '\\t' + by.main_sense(Language.EN).full_lemma,\n",
    "          edge.pointer, edge.id_target, bn.get_synset(edge.id_target).main_sense(Language.EN).full_lemma, sep=' - ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['affecting', 'poignant', 'touching', 'poignant']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(chain.from_iterable([syn.lemma_names() for syn in wn.synsets('poignant', pos=wn.ADJ)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eccentric\n",
      "eccentric\n",
      "eccentric\n",
      "elongated_orbit\n",
      "orbital_eccentricities\n",
      "parabolic_and_hyperbolic_orbits\n",
      "kookiness\n",
      "quirkiness\n",
      "Eccentrism\n",
      "eccentric_behaviour\n",
      "eccentrics\n",
      "quirkiness\n",
      "Quirky\n",
      "Quirky.com\n",
      "Undercurrent\n",
      "Wackiness\n",
      "Wacko\n",
      "Wacky\n",
      "distance\n",
      "distance\n",
      "geodetic_distance\n",
      "diameter\n",
      "diameter\n",
      "geodesic_distance\n",
      "graph_diameter\n",
      "graph_metric\n",
      "Graph_radius\n",
      "pseudo-peripheral_vertex\n",
      "radius\n"
     ]
    }
   ],
   "source": [
    "w = 'eccentricity'\n",
    "\n",
    "for s in bn.get_senses(w, from_langs=[Language.EN], poses=[POS.NOUN]):\n",
    "    if w not in str(s.lemma).lower():\n",
    "        print(s.lemma)\n",
    "        \n",
    "# bn.get_synsets(w, from_langs=[Language.EN], poses=[POS.NOUN])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonym Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import names\n",
    "\n",
    "NO_MATCH = set()\n",
    "no_match_counter = 0\n",
    "word_counter = 0\n",
    "weird_cases = set()\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "ones = {\n",
    "    0: '', 1: 'one', 2: 'two', 3: 'three', 4: 'four', 5: 'five', 6: 'six',\n",
    "    7: 'seven', 8: 'eight', 9: 'nine', 10: 'ten', 11: 'eleven', 12: 'twelve',\n",
    "    13: 'thirteen', 14: 'fourteen', 15: 'fifteen', 16: 'sixteen',\n",
    "    17: 'seventeen', 18: 'eighteen', 19: 'nineteen'}\n",
    "tens = {\n",
    "    2: 'twenty', 3: 'thirty', 4: 'forty', 5: 'fifty', 6: 'sixty',\n",
    "    7: 'seventy', 8: 'eighty', 9: 'ninety'}\n",
    "illions = {\n",
    "    1: 'thousand', 2: 'million', 3: 'billion', 4: 'trillion', 5: 'quadrillion',\n",
    "    6: 'quintillion', 7: 'sextillion', 8: 'septillion', 9: 'octillion',\n",
    "    10: 'nonillion', 11: 'decillion'}\n",
    "\n",
    "\n",
    "def say_number(i):\n",
    "    \"\"\"\n",
    "    Convert an integer in to it's word representation.\n",
    "\n",
    "    say_number(i: integer) -> string\n",
    "    \"\"\"\n",
    "    if i < 0:\n",
    "        return _join('negative', _say_number_pos(-i))\n",
    "    if i == 0:\n",
    "        return 'zero'\n",
    "    return _say_number_pos(i)\n",
    "\n",
    "\n",
    "def _say_number_pos(i):\n",
    "    if i < 20:\n",
    "        return ones[i]\n",
    "    if i < 100:\n",
    "        return _join(tens[i // 10], ones[i % 10])\n",
    "    if i < 1000:\n",
    "        return _divide(i, 100, 'hundred')\n",
    "    for illions_number, illions_name in illions.items():\n",
    "        if i < 1000**(illions_number + 1):\n",
    "            break\n",
    "    return _divide(i, 1000**illions_number, illions_name)\n",
    "\n",
    "\n",
    "def _divide(dividend, divisor, magnitude):\n",
    "    return _join(\n",
    "        _say_number_pos(dividend // divisor),\n",
    "        magnitude,\n",
    "        _say_number_pos(dividend % divisor),\n",
    "    )\n",
    "\n",
    "\n",
    "def _join(*args):\n",
    "    return '-'.join(filter(bool, args))\n",
    "\n",
    "def convert_to_case(old, new):\n",
    "    global weird_cases\n",
    "    if old.isupper():\n",
    "        return new.upper()\n",
    "    if old.islower():\n",
    "        return new.lower()\n",
    "    if old.istitle():\n",
    "        return new.title()\n",
    "    weird_cases.add(old)\n",
    "    return new\n",
    "\n",
    "def random_wordswap(iterable):\n",
    "    return [ find_random(x) for x in iterable]\n",
    "\n",
    "def find_random(word):\n",
    "    global vocab\n",
    "    return convert_to_case(word, random.choice([w for w in vocab if w != word.lower()]))\n",
    "\n",
    "def find_replacement(word, pos=''):\n",
    "    global twitter_ids\n",
    "    \n",
    "    # Get list of appropriate twitter aliases? and names?\n",
    "    # Get list of punctuation\n",
    "    quotes = [ \"'\", \"''\", \"`\", \"``\", '\"']\n",
    "    brackets = [\"(\", \")\", \"{\", \"}\", \"[\", \"]\", '/']\n",
    "    punct = [ '.', '!', '?', ',']\n",
    "    breaks = ['-', '--', ',', ':', ';']\n",
    "    \n",
    "    if word[:12] == \"http://t.co/\" : ##URL: \n",
    "        return  word[:-8] + ''.join(random.choice(string.ascii_letters + string.digits) for i in range(8))\n",
    "    \n",
    "    if word[:13] == \"https://t.co/\" : ##URL:\n",
    "        return  word[:-8] + ''.join(random.choice(string.ascii_letters + string.digits) for i in range(8))\n",
    "    \n",
    "    if word[0] == '#':\n",
    "        return('#' + find_replacement(word[1:]))\n",
    "    \n",
    "    if word[0] == '@': # twitter Id\n",
    "        return random.choice([t for t in twitter_ids if t != word])\n",
    "    \n",
    "    if pos == 'DT':\n",
    "        dets = ['a', 'an', 'the', 'this', 'that']\n",
    "        return convert_to_case(word, random.choice([d for d in dets if d != word.lower()]))\n",
    "    \n",
    "    if pos == 'WDT':\n",
    "        wdts = ['that', 'what', 'whatever', 'which', 'whichever']\n",
    "        return convert_to_case(word, random.choice([d for d in wdts if d != word.lower()]))\n",
    "    \n",
    "    # if pos == 'PRP$':\n",
    "    #     prps = ['her', 'his', 'mine', 'my', 'our', 'ours', 'their', 'your']\n",
    "    #     return random.choice([d for d in prps if d != word.lower()])\n",
    "    \n",
    "    # if pos == 'PRP':\n",
    "    #     prps = ['hers', 'herself', 'him', 'himself', 'hisself', 'it', 'itself', 'me', 'myself', 'one', 'oneself', 'ours', 'ourselves', 'ownself', 'she', 'theirs', 'them', 'themselves', 'they', 'us']\n",
    "        \n",
    "    if pos == 'NNP': # Proper noun\n",
    "        if word[-2:] == \"'s\":\n",
    "            return convert_to_case(word[:-2], random.choice([names.get_first_name(), names.get_last_name()])) + \"'s'\"\n",
    "        else:\n",
    "            return convert_to_case(word, random.choice([names.get_first_name(), names.get_last_name()]))\n",
    "    \n",
    "    \n",
    "    if word in quotes:\n",
    "        return random.choice([d for d in quotes if d != word.lower()])\n",
    "    \n",
    "    if word in brackets:\n",
    "        return random.choice([d for d in brackets if d != word.lower()])\n",
    "    \n",
    "    if word in punct:\n",
    "        return random.choice([d for d in punct if d != word.lower()])\n",
    "    \n",
    "    if word in breaks:\n",
    "        return random.choice([d for d in breaks if d != word.lower()])\n",
    "    \n",
    "    if word.isnumeric():\n",
    "        return say_number(int(word))\n",
    "        \n",
    "    # Collect wordnet synonyms\n",
    "    options_wn = [ w.replace(\"_\", \"-\") for w in list(chain.from_iterable([syn.lemma_names() for syn in wn.synsets(word.lower(), pos=get_wordnet_pos(pos))])) if w != word]\n",
    "    \n",
    "    if options_wn == []:\n",
    "        options_wn = [ w.replace(\"_\", \"-\") for w in list(chain.from_iterable([syn.lemma_names() for syn in wn.synsets(word.lower())])) if w != word]\n",
    "    \n",
    "    # Collect synonyms from PPDB\n",
    "    if pos != '':\n",
    "        try:\n",
    "            options_ppdb = clean_ppdb_synonyms[word.lower()][pos]\n",
    "        except KeyError:\n",
    "            options_ppdb = []\n",
    "    else:\n",
    "        try:\n",
    "            options_ppdb = clean_ppdb_synonyms_XXL[word.lower()]\n",
    "        except KeyError:\n",
    "            options_ppdb = []\n",
    "            \n",
    "    # Babelnet?? -- REQ PYTHON 3.8\n",
    "        \n",
    "    full_set = options_wn + options_ppdb\n",
    "    \n",
    "    try:\n",
    "        return convert_to_case(word,random.choice(full_set))\n",
    "        \n",
    "    except IndexError:\n",
    "        \n",
    "        if word[-1] in breaks:\n",
    "            return find_replacement(word[:-1]) + word[-1]\n",
    "            \n",
    "        if word[0] in breaks:\n",
    "            return word[0] + find_replacement(word[1:])\n",
    "        \n",
    "        if word[-1] in punct:\n",
    "            return find_replacement(word[:-1]) + word[-1]\n",
    "                    \n",
    "        if word[0] in punct:\n",
    "            return word[0] + find_replacement(word[1:])\n",
    "        \n",
    "        if word[-1] in quotes:\n",
    "            return find_replacement(word[:-1]) + word[-1]\n",
    "            \n",
    "        if word[0] in quotes:\n",
    "            return word[0] + find_replacement(word[1:])\n",
    "        \n",
    "        if word[-1] in brackets:\n",
    "            return find_replacement(word[:-1]) + word[-1]\n",
    "            \n",
    "        if word[0] in brackets:\n",
    "            return word[0] + find_replacement(word[1:])\n",
    "        \n",
    "        if word[-1] == \"%\":\n",
    "            return find_replacement(word[:-1]) + '%'\n",
    "        \n",
    "        ### Try to parse by hyphens\n",
    "        if '-' in word:\n",
    "            parts = word.split('-')\n",
    "            for i,p in enumerate(parts):\n",
    "                n = find_replacement(p)\n",
    "                if n != p:\n",
    "                    return '-'.join(parts[:i] + [n] + parts[i+1:])\n",
    "\n",
    "                \n",
    "        if '/' in word:\n",
    "            parts = word.split('/')\n",
    "            for i,p in enumerate(parts):\n",
    "                n = find_replacement(p)\n",
    "                if n != p:\n",
    "                    return '/'.join(parts[:i] + [n] + parts[i+1:])\n",
    "                \n",
    "        if '.' in word:\n",
    "            parts = word.split('.')\n",
    "            for i,p in enumerate(parts):\n",
    "                if p != '':\n",
    "                    n = find_replacement(p)\n",
    "                    if n != p:\n",
    "                        return '.'.join(parts[:i] + [n] + parts[i+1:])\n",
    "    \n",
    "\n",
    "        \n",
    "        # #### Try to parse TextLikeThis\n",
    "        # if len(word) > 1 and not word.isupper() and any(ele.isupper() for ele in word):\n",
    "        #     parts = re.findall('[a-zA-Z][^A-Z]*', word)\n",
    "        #     for i,p in enumerate(parts):\n",
    "        #         if p != '':\n",
    "        #             n = find_replacement(p)\n",
    "        #             if n != p:\n",
    "        #                 return ''.join(parts[:i] + [n] + parts[i+1:])\n",
    "        \n",
    "        ## check less good fits\n",
    "        try:\n",
    "            return convert_to_case(word,random.choice(clean_ppdb_synonyms_XL[word.lower()][pos]))\n",
    "        except (KeyError, IndexError):\n",
    "            try:\n",
    "                return convert_to_case(word,random.choice(clean_ppdb_synonyms_XXL[word.lower()]))\n",
    "            except ( KeyError, IndexError) :\n",
    "                if word[-3:] == 'ish':\n",
    "                    return find_replacement(word[:-3]) + 'ish'\n",
    "                if word[-4:] == 'ness':\n",
    "                    return find_replacement(word[:-4]) + 'ness'\n",
    "                if word[-4:] == 'less':\n",
    "                    return find_replacement(word[:-4]) + 'less'\n",
    "                # if word.istitle(): # implies proper noun\n",
    "                #     return random.choice([names.get_first_name(), names.get_last_name()])\n",
    "                return word\n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "def wordswap(iterable):\n",
    "    global pos_tags\n",
    "    global NO_MATCH\n",
    "    global no_match_counter\n",
    "    global word_counter\n",
    "    out = []\n",
    "    for i,x in enumerate(iterable):\n",
    "        word_counter += 1\n",
    "        new = find_replacement(x, pos_tags[i])\n",
    "        if new == x:\n",
    "            NO_MATCH.add((x, pos_tags[i]))\n",
    "            no_match_counter += 1\n",
    "        out.append(new)\n",
    "                \n",
    "    return out\n",
    "        \n",
    "        \n",
    "def create_random_masks(example):\n",
    "    masks = {}\n",
    "    noisy = {}\n",
    "\n",
    "    ### Start with everything\n",
    "    prop=1\n",
    "    mask = np.ones(len(example), dtype=int)\n",
    "    mask.astype(bool)\n",
    "    updated_lasttime= True\n",
    "\n",
    "    for additional in [.05]: #,.05,.1,.1,.2,.25]:\n",
    "        prop -= additional\n",
    "        if not updated_lasttime:\n",
    "            amt = old + additional\n",
    "        else:\n",
    "            amt = additional\n",
    "            \n",
    "        to_remove = round(amt * len(mask))\n",
    "        \n",
    "        if to_remove == 0:\n",
    "            updated_lasttime = False\n",
    "            old = additional\n",
    "        else:\n",
    "            updated_lasttime = True\n",
    "            mask = obscure_less(mask, to_remove)\n",
    "            \n",
    "        # masks[f'random_{prop*100:2.0f}'] = mask\n",
    "        # noisy[f'random_token_{prop*100:2.0f}'] = ' '.join(np.where(mask, '{TOKEN}' , example))    \n",
    "        # noisy[f'random_charswap_{prop*100:2.0f}'] = ' '.join(np.where(mask, charswap(example) , example))    \n",
    "        noisy[f'random_synonym_{prop*100:2.0f}'] = ' '.join(np.where(mask, wordswap(example) , example))    \n",
    "    \n",
    "    return noisy\n",
    "\n",
    "\n",
    "def match_pos_token_to_original(pos_tokens, raw_orig, pos_tags):\n",
    "    orig = []\n",
    "    for word in raw_orig:\n",
    "        if word != \"\":\n",
    "            orig.append(word.strip())\n",
    "    pos_idx = 0\n",
    "    last_pos_idx = 1\n",
    "    orig_idx = 0\n",
    "    \n",
    "    orig_to_pos_mapping = {}\n",
    "    orig_idx2token = {}\n",
    "\n",
    "    while pos_idx < len(pos_tokens) and orig_idx < len(orig):\n",
    "\n",
    "        current_orig = orig[orig_idx]\n",
    "        current_pos = pos_tokens[pos_idx]\n",
    "        orig_to_pos_mapping[orig_idx] = [pos_idx]\n",
    "        \n",
    "        pos_idx += 1\n",
    "        orig_idx2token[orig_idx] = current_orig\n",
    "        if current_pos != current_orig:\t\t\t\n",
    "            combined = current_pos\n",
    "            last_pos_idx = pos_idx\n",
    "            while last_pos_idx < len(pos_tokens):\n",
    "                next_part = pos_tokens[last_pos_idx]\t\t\t\t\n",
    "                combined += next_part\n",
    "                orig_to_pos_mapping[orig_idx].append(last_pos_idx)\n",
    "                if combined == current_orig:\t\t\t\t\t\n",
    "                    pos_idx = last_pos_idx + 1\n",
    "                    break\n",
    "                else:\n",
    "                    last_pos_idx += 1\n",
    "\n",
    "        orig_idx += 1\n",
    "        \n",
    "    pos_to_drop = [\"$\", '', \"(\", \")\", \",\", \"#\", \"POS\", \"--\", \".\", \":\", \"''\", '``']\n",
    "\n",
    "    new_pos_tags = []\n",
    "    for k in orig_to_pos_mapping.keys():\n",
    "        if len(orig_to_pos_mapping[k]) == 1:\n",
    "            new_pos_tags.append(pos_tags[orig_to_pos_mapping[k][0]])\n",
    "        else:\n",
    "            to_add = []\n",
    "            for i in orig_to_pos_mapping[k]:\n",
    "                if pos_tags[i] not in pos_to_drop:\n",
    "                    to_add.append(pos_tags[i])\n",
    "            if len(to_add) == 1:\n",
    "                new_pos_tags.append(to_add[0])\n",
    "            else:\n",
    "                new_pos_tags.append('')\n",
    "    \n",
    "    return new_pos_tags\n",
    "\n",
    "\n",
    "def insert_random_noise(example):\n",
    "    global pos_tags\n",
    "    global tokens\n",
    "    nltk_tokens = nltk.word_tokenize(example['text'])\n",
    "    pos_tags = [val[1] for val in nltk.pos_tag(nltk_tokens)]\n",
    "    \n",
    "    tokens = example['text'].split()\n",
    "    \n",
    "    pos_tags = match_pos_token_to_original(nltk_tokens, tokens, pos_tags)\n",
    "        \n",
    "    noise = create_random_masks(tokens)\n",
    "    return noise #example | noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 63/63 [00:00<00:00, 257.25 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16187739463601533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out = dataset.map(insert_random_noise)\n",
    "\n",
    "print(no_match_counter / word_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NO_MATCH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [\u001b[39mprint\u001b[39m(n[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m NO_MATCH]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NO_MATCH' is not defined"
     ]
    }
   ],
   "source": [
    "[print(n[0]) for n in NO_MATCH]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'InternationalDirectionerDay'\",\n",
       " '+',\n",
       " '..',\n",
       " '0-0',\n",
       " '8:00',\n",
       " 'BusinessWeek',\n",
       " 'DiViNetworks',\n",
       " 'ICONiac',\n",
       " 'ICONiacz',\n",
       " 'McBain',\n",
       " 'PSSAs',\n",
       " 'PSSAs!',\n",
       " 'RT/follow',\n",
       " 'RamCharan',\n",
       " 'WorldStarHipHop',\n",
       " '|'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weird_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACE scarce trust Edmund Bennett be beginning tabu of that excerpt car this-evening #terrierball #htafc #dontfancyArfieldingoal',\n",
       " \"Follow this motion-picture day #The70''s' along #OVTV and be sunnily startled to watch Song Brooks (Bathroom, #OLTL) in it.\",\n",
       " 'ATOMIC-NUMBER-53 wish that @michaelaconlin: telecasting http://t.co/c2d35dek ERROL -- quaternary Live Zachary twenty-seven/31 -- We be + Wicks upward a she',\n",
       " \"THE winning for #LFC five Olson testament see them pose oftheir outside full-point check for that overall of utmost mollify. And we take to gain tomorrow''s' ballgame.\",\n",
       " '@pariahbeats thanksgiving! Laura the email to info@kayafestival.commanders.uk consider booking. Pick-Up uyou fetch Smith Noe this-evening , flummox striation!',\n",
       " 'Non an Arsehole hater only pleased to go-steady them add-up backward from Langley pile last-place nighttime v Leray to acquire zero-0. Wilda show hangingwith a Harry Luke as-well.',\n",
       " \"Thither be even-so just-the-ticket uncommitted for a Nagle nine venire at Paleyfest. Allow''s' cook #Hall that sell away assembly! Stewart tix herein: http://t.co/MqQX9Cta\",\n",
       " \"Glad Valentines daytime ? Don't stop upwardly whereby an sunlight don't glow attain trusted to start toyour particular soul whichever talent from Desilets Tan .!!!!!!\",\n",
       " \"Passionate about CNIS' knead, live the repp atomic-number-85 Alcantara Olsen Hoover inch ofyour home along Jeffery eight : Winkler seven-hundred-twenty-five Dustin Manske Carl LAVETTE\",\n",
       " \"Valentine''s' exactly keep fuck-off considerably for pine-tree-state. Watch a dawdler to JOSEPH JOHNSTON GRISWOLD DAN sixteen JENNIFER three and it transmit along BAIRD Flowers thirteenth! YES!!!\",\n",
       " \"Passionate or-so CNIS' lick, exist an repp astatine Samuel Luisa Pearl in toyour home along Bailey eight : Bookman Mccray Michael one-thousand-nine-hundred-seventy-six Stein Stewart Garcia\",\n",
       " \"Don't expect til this final mo to scripture meetyour stall and nonplus tix for Margaret along Davis eleventh...you will know to repent it!\",\n",
       " \"Passionate nigh CNIS' form, constitute that repp at Fifer Betty Arana inch thatyour town along Pena eight , Saint Lincoln 116 Shady Brian\",\n",
       " '@hamilton_elle non restrict to NEIL ; reckon what hap to this mightily Ethel Garret. Immediately this eunich. Contented Mon, glisten human-face.',\n",
       " 'Get-Laid an natural-endowment exhibition let die to this hoops tourney along Serrano at Gayle vapid to plunk-for kenston timber',\n",
       " \"Get perilously obsessed hangingwith single of tomorrow''s' Arfman songwriters. I'm certain uyou derriere check wherefore. http://t.co/jOOZG5Y4\",\n",
       " \"good documental characteristics hold-up to Save Face, hahahaa simply nonpareil opsi from this selection, this''s' information-technology! #Oscar\",\n",
       " 'A #Academy-Award for a good documental brusque goes to Thomas Denson and Betty Jaclyn for Save Case',\n",
       " 'Boyle Hack Chinoy, we be majestic of uyou, Save Face, win astatine that 84 Woods indium Tolbert Gary Holtmeier more: http://t.co/0nPi4xVt',\n",
       " \"@ali_liz1 Hawai'I, my figure comprise Dayle, abdul- you're my stirring , :) (RT/Follow ifs uyou understand this?)47\",\n",
       " 'Potent tax-revenue emergence astatine Tudou, merely an fellowship keep along bleed heavily -- http://t.co/uJDBnL8b',\n",
       " \"Information-Technology''s' non Simonds Howland''s' fracture that Reeve Mitzi take ego mastery payoff, and here's an way-of-life helium get provoke therewith this military-recruit he own derive inward\",\n",
       " 'Rosen Rage: Cieslinski Cardenas nowadays five/4 (tangible price be four/6) close-to Hamilton Hensley wins Byrne Rolando upcoming Tues.',\n",
       " \"Westover derby Gary Costello have an cheek accidental-injury and won't play inwards tomorrow''s' tertiary and determine matchless twenty-four-hour-period concluding against Baron S.Lanka. #cricket\",\n",
       " 'slant waiting to figure that iPad PROUTY ,.',\n",
       " '@loubezjakfmn: skillful circumstances along that filth & MILLER from a Tyrone kinfolk',\n",
       " 'BRUMAGHIM jitney Kercheval Keane say Josiah Wayne be this slap-up participant only helium await Dennis to follow unitedly and bring comfortably Thurs.',\n",
       " 'At my last FRUMAN revelation. Lend along that gurgle! #IWD',\n",
       " \"It''s' Wesley Willey eight. Whatever buns only-if entail unmatched stuff. Felicitous ANNA one-half an planet :-/ Show around it hera: http://t.co/9pepOJOH #WomensDay #IWD\",\n",
       " 'World-Health-Organization break-away this creation...girls! Happy IWD!!',\n",
       " \"Mundane cue selves uyou be a dependable! Merry Clement Housewives''s' day , http://t.co/1TFK4ZZf\",\n",
       " \"For Paul Girl''s' someday, allow''s' non draw-a-blank trans*women and the who don't survey an sex binary-program!\",\n",
       " \"Ifs this Johnny yet feature Catherine Farmar,Trevor Ariza,&Shannon Spivey I'd cost find-out them ..II dont like that Lakers only they equal nurse.\",\n",
       " '@DerrickWalton10 Victor good-afternoon Williams wishing uyou this salutary twenty-four-hour-period and an playfulness fill-up week atomic-number-85 Diaz tomorrow',\n",
       " \"@DiscoMundial Caras gladiolus you're have that just metre in Rice truelove! :{ & awsome that uranium volition follow Denise therewith the-states tomorrow! Xo\",\n",
       " \"They didn't squawk youwhen Carrie Janice run this bollock to Mckeon Farmar to deliver-the-goods a poker...but Jackson can't go inward an fourth stfu\",\n",
       " 'Astatine this LEWIS paribas Lewis inward an red-hot sunrise. ATOMIC-NUMBER-53 recognize a of uyou would have-sex a conditions and an lawn-tennis.',\n",
       " 'Brown my Wells along simply day :) @TheFaversham #Tangent for #FREE @RealWizKhalifa the Daniel Suntan beauty-parlor inward Gramby LOUISE three/10! Name eight-hundred-sixty-six-970-8267',\n",
       " 'Uyou cost come-out of fortune ifthe uyou wanna to travel to this TEFFT Norman Cristina on Rico every-bit slate be non obtainable. Sami for Matthew twenty-four-hours school-term #bnppo12',\n",
       " 'QT @justinbieber Atomic-Number-49 this master selective-service of an seventh sourcebook, Hill Debra survive this Sturgill of Hogwarts. #HappyBirthdayRemusLupin',\n",
       " 'JANICE DEANNA DAVIDSON IRENE Thomas Dorothy hurt a surprisal precocious outlet from this CHARLES Robert Kathy inwards Allison Herbert-George-Wells. http://t.co/m18yIWvC',\n",
       " 'Observe \"The Pre-Announcement\" on @WeAreFarmers or-so #Thaumaturgy President-Johnson. Awe-Inspiring documentary-film- angstrom moldiness watchdog.',\n",
       " 'Colan, James-Harvey-Robinson, St.-Simon, Moldoff, and nowadays Moebius, oftheir loss scarce mean their hooey be to-a-greater-extent \"uncollectible.\" Think around that, won\\'t uyou?',\n",
       " 'and an braggy surprising of an Clint David represent equally many ampere nineteen occupy along Vanessa in that Walden Handicap!',\n",
       " \"ACE call-for to catch-some-z's ingest that large clarence-day read that Latoya tomorrow. Lester a cashee equal a life-sentence for four hour\",\n",
       " \"@pjforest0712 Back this former Ruddy. Mistakes. That style favorable thirty-one look along Scotsirish, sure-enough Kimball won't permit maine depressed? Clifford own the punting along the final.\",\n",
       " \"Fundament SINGLE just aver that ONE sleep-with Carson Lucero aka Virginia Easton! Melissa to Michael Charles-Ii!!! But they don't involve to make-for Natali! #gh\",\n",
       " \"Mcintosh come-back the Marta night at eight along CNBC. SINGLE learn it astatine Jacqueline simply I'm find-out information-technology once-again! #anniesboobs\",\n",
       " \"The difficult winning for u present-day! Never comprise in a tractor-trailer''s' inwards DARIN Knight Outdoors! Johnson teaming along Michael :-]\",\n",
       " 'twenty-five-year-old Rodriguez Sierra gain Gregory -- Manis Cooper gain an Iditarod Dial Conley Rodgers Irene Tues, become an a.y... http://t.co/XJOaeXzD',\n",
       " 'twenty-five-year-old Richard Welk wins Freddy -- Nicole Burke advance that Harley Thorn Swain Schneider Michael Wednesday, become !.. http://t.co/uQ3PMRAy',\n",
       " 'Alaska musher, twenty-five, immature to triumph Iditarod-Trail-Dog-Sled-Race: Dallas Posey gain that Patterson Reed Chico Mcelhenney Valencia along Colleen tied... http://t.co/H5liBSox',\n",
       " '#sport Church musher, twenty-five, new to win Iditarod-Trail-Dog-Sled-Race: Mabel Allan win this Sharon Evans Joleen Frein Haskins along Tuesday... http://t.co/p9BBJWUM',\n",
       " 'Noah young Gandolfi admirer: Hernandez Garcia won this James Viramontes Grover Deborah Barris Thursday, suit that youn... http://t.co/1Ong6Khc',\n",
       " 'STS SMITH : Alaska musher, twenty-five, young to pull-ahead Woods , Peter Sara bring-home-the-bacon this Tana Stephen Porter Oscar Sherry along Tuesday... http://t.co/CvTHL7Ya',\n",
       " \"Tudou Morris Get Worldview''s' Secure Business-Deal hangingwith Erica -- Carlson (web-log) http://t.co/bKdaht7I\",\n",
       " 'Polarity of an time? MILLICK Charles -- Williams Luz terminate information-technology famed print variation http://t.co/Cz9PcXLj',\n",
       " '@CarlJWood I\\'m meritless ONE insufficiency an draw skills to compensate court to such that enceinte illustrator a Mary \"Moebius\" Giraud along Tues Sketch two.',\n",
       " 'Suzanne Jeanne entail atomic-number-2 have that involvement atomic-number-49 #Huskers hoop coach book-of-job. See PEARSON Times ,.. http://t.co/wWmYhsls',\n",
       " \"#eleven Robert Jeanette receive outwit #one Anna Cardenas seven-6, three-6, seven-6 inwards a PERRY Lorraine Open. Scott to a humanity''s' meridian 10 #JohnIsner.\",\n",
       " 'Nita Denk live a heavy slew for maine, the roll-in-the-hay exist render, simply Fasheh Nathan Mayo along an former hired-man James thats my second bday !',\n",
       " '@buzzkut9 Scott Pickron engender interahamwe sufficiently forward to set-about an LAURA patrolling away to stupefy u an second one-half terminated-going two a Katelyn Betty company &amp; baffle rummy',\n",
       " 'Compliments I be hangingwith this wide-cut Johnny Krause along that Dubin particularly Adams Janice red-cent would of embody inestimable yaqoub sense maine #enoughsaid #realtalk',\n",
       " 'Knaus, Isabelle deliver-the-goods appealingness; the respite forthcoming, point rejuvenate: Sportswoman three time-of-day, one second agone From the import N.J... http://t.co/6FJggL9m',\n",
       " 'Watch ours schooling along JOHNSON Nelson tomorrow therewith Hamilton Garret. Naomi serve america lofty! @EmRusciano @ShannonLeto #edchat http://t.co/Ys869IlT',\n",
       " 'Thursday play for scarcely the few TILLIE along Doris twelve therewith Worrell King, Crystal AMANDA & more-than + Larry Kyle & Barton An Snitch! http://t.co/vMMkM1e5',\n",
       " \"hollywoodcrush: 'My Ford Williams Jacobson sixteen Moore three' premiere tonight! Degenfelder for an mass-murder therewith that snip: http://t.co/ucNTXtD1\",\n",
       " '@nyTimes William beour plot hoosier-state bridge-player embody 5 Boulton!!!!!. six cursor has!..Playing Trisha utmost secret-plan of this season!.#hopeless',\n",
       " \"@itsAishaAmiri Jacqueline twentieth uyou roast ;} Taylor Smith special along myx idk what's fourth-dimension tho @ChewB30 @__wilo___ @Kent_BA\",\n",
       " 'intimately diddlyshit... aren\\'t we plainly presuppose to rouse upward to \"Kony two-thousand-twelve\" posting all-over tomorrow...? beaver-state did that totally only decreased herthrough?',\n",
       " 'SIGRID Cricket: Pattinson crataegus-oxycantha constitute save for AN tour in-front of Ash: Smith Alec crataegus-laevigata non take-a-shit information-technology to this HODAK circuit o.a... http://t.co/ZwAT7thD',\n",
       " 'Callis Steven may non make it to this KELLS turn of Rex disdain his expect convalescence equally this selector-switch speculate ,.. #Australasia #CRICKET',\n",
       " 'http://t.co/q5ZycjkV We cost get prepared for that expectant #Bollyfit Erin Trost tomorrow with beour tutors!',\n",
       " \"Just get that call from a Johnson awarding fest--we're upward for two prize, ceremonial to kick forth hoosier-state Dwayne Lola nineteenth.... http://t.co/ECVlqRoT\",\n",
       " \"@ItsARob88 lead to do-work along Harling and person had nebulizer paint 'Kony two-thousand-twelve' along a face of purple gamy shoal haha pathetic endeavor\",\n",
       " 'OMG. Robinson sport inward Evelyn new MV? Whitehead night?! FYEAH.',\n",
       " \"@dandundas leadersince Candace prove uyou lovemaking, hand that backward to a billet where it a start-up and do at Allderdice''s' promenade along Norma twenty-four\",\n",
       " \"@Diarmuidm77lfc4 and I sense an exacting identical mode approx William haha, it my second preferent projection he''s' liberated (behindhand K&OJ). helium making estimable euphony\",\n",
       " '@Kdizzle3434 And-Then happy 1 get to meet uyou indiana that frame lowest Barkley @Lararamos24 for Soundboard!',\n",
       " '@htansatlanta: @ReturnOfTheMac6 @ReturnOfTheMac6 @loumongello ACE call-back that a 2nd billet along Robert hold a point along Eurovision! ;[',\n",
       " 'Final daylight of Jewel tomorrow :{ #y-yes #yeahbuddy ,!!',\n",
       " \"IODIN don't the-likes-of testing I'm delighted I don't take-in to disturb most this KAUBLE any-longer along english-hawthorn.8th ]: pass-on that shxt .\",\n",
       " \"End-Up take an Patricia mathematics prep... [-.-) ...Brainless seventh class maths ACE don't yet reminisce embody along information-technology... #Prep\",\n",
       " '@YouTube Understandwhat an disgrace roughly Punchestown. Unlikely to raceway atomic-number-85 an tomorrow. ONE ingest two away every-bit comfortably in Doris and Shawn (two miler!!!)',\n",
       " 'Mark divinity tomorrow be that lowest mean-solar-day for PSSAs!',\n",
       " \"Shields be still cash to bring-home-the-bacon the Fryer Sean hangingwith @karaotr Estes Glad Webb to maine #sheisthefemalebigbuck''s'\",\n",
       " \"Toller three of Paulette (res-publica test for eleventh ratings). Tina questions and sci scenario depart significantly; shouldn't they exist select a selfsame examinations? #pssa\",\n",
       " '#THSK7thanniversary A first birdsong contribute maine to MILLER and pretend maine turn an Stewart be Wherefore set ANE devolve indiana sexual-love with uyou?',\n",
       " '@LilTunechi Uyou bequeath apprize this.. Danny lunch chocolate: Ordinary cupful indiana b/universal-gravitational-constant and and-so a ANGELA of java-based. Yowza. http://t.co/1B8GCtKg',\n",
       " \"RamCharan''s' #Racha succesfully fill-in 3weeks hangingwith disc collections,into fourth workweek! Immediately!!\",\n",
       " '@REAMER_72 be a veridical mete-out. ONE cannot hold-back to habituate it tomorrow night. BARNES Competitions. Watch it along KIRBY Timothy Mckay astatine 8:00pm autopsy',\n",
       " '@LadySmaug Linda represent just-now too beneficial, believe it may fight the small hangingwith the condition! Halstead along Kennon Shannon HUBBARD Collen ;/ haha',\n",
       " '[#Happy7thAnniversaryTHSK] \"GIVE-WAY OUT!\" be the 29e bingle of THSK that beat the log of Elton Saint-John-The-Apostle for xii old-age and four-person person-months.',\n",
       " \"Ultimately incur to examine This Thomas #Magic ...selfsame invigorate tale to ne'er hand upwardly...God volition help that thing understandwhat that luck crataegus-oxycantha equal\",\n",
       " \"@meow_x @CarlJWood UNITY call-back Ruth get enlist atomic-number-49 sixth operating-theater seventh rd. Had an bully Rebecca Owen that''s' phone nigh redress!!!\",\n",
       " 'Williams Griffin David (1D-Ohio), who drop-off inward that late Robert elementary selection along Angela sixth, due to... http://t.co/I0tUU2Ri',\n",
       " 'Whitney understandwhat an plot a fox deliver-the-goods inwards that rump of that tenth past an walk slay LAYNE aside Maria Kemp.',\n",
       " '1 like an @DiscoMundial tv http://t.co/HQhsdLPK Zimmerman Watson Derek Ward seven two-thousand-twelve iPad three this new ipad Full Arthur',\n",
       " 'Congratulations to this challenging working @itsAishaAmiri along toss-off his Jose Pederson and fool-away upwards to an quaternary Rebecca of an #NFLDraft!',\n",
       " \"whoreson that spend-a-penny pine-tree-state polish-off: Adult-Male what plain near Harris Mothers''s' Dennis and don't pull-in Robin men's Kenneth represent Ebony mid-19th. ANDERSON GREIG\",\n",
       " 'Government of Canadian chairman Chambers Mcknight do to verbalise Shaffer twenty-four, seven post-meridiem, astatine that Jerry Gary indium Sarnia. Matter, save the Felton Water.',\n",
       " \"@__wilo___ Joyner Smith [ @espn vocalization / can't hold-back to ballot tomorrow &amp; pop-off to that award &amp; touch this casting of Pocius Crowley Alan Veshedsky sixteen!\",\n",
       " 'Chaney first Marie fill-in &amp; John make-up pleased therewith an output',\n",
       " 'Cheerful May Severson Folk! Make-Up information-technology fete a natural-spring sidereal-day of an year-to-year cycles/second, beaver-state scorch Music James and speak approximately IWD, Have that Peachy Anniversary!',\n",
       " 'STERMER @ Joiner of Can president Landon Coleman hit route to talk along safeguards this Wurdeman Lake; eight-city duty-tour testament watch hers indium #hamont Ramirez sixteen',\n",
       " 'Never live Moebius although. Face ilk that refrigerate performer! Why act altogether an majuscule performer constantly croak in Edge?!',\n",
       " \"@Diarmuidm77lfc4 Alas, non still. Require to disappear to Johnson astatine Geraldine inward March, only couldn't realize information-technology. Joseph shortly. Markow happy for uyou! :/\",\n",
       " \"Excited for that GREENE Harris tomorrow. I palpate corresponding I'm diddle in that lame. ATOMIC-NUMBER-63, let generate information-technology exercise.\",\n",
       " '@guardiannews ah single wanna to encounter Linder over-again tomorrow just i require to go-away to church-service haha',\n",
       " 'Buzzword eternal-sleep. Taylor brain. Look-Out \"The Announcement\" for the fifth part-time foot. @Kenny_Wallace #handsdown one of the virtually aroused feature legends forever',\n",
       " \"Stone nighttime Carolyn Stephanie (Puncher Minardi Wanda Mary hoosier-state an Trounce) remind me how's a-great-deal 1 love Yuko Kanno''s' medicine. Such this fantastic composer.\",\n",
       " \"@__wilo___ oo nga eh :} may visualise sana'a ako hangingwith PREPIX!!!!\",\n",
       " \"Truly ought set-out this sopor since 1 receive this ROSS war-paint test tomorrow. Merely I'm non, this normal.\",\n",
       " \"Information-Technology''s' House Project's 2d monthsary now! I intend that''s' rationality decent to voting and quest to realize Welle ORTIZ BUTLER now! :(\",\n",
       " 'budorcas-taxicolor my mathematics HUMMEL tomorrow ! th peace-garden-state friday helpin away astatine this especial olympics , hopin my good admirer be in-that-respect , /: #Winnin ,',\n",
       " \"@DIGIADVANSOUTH : Hope uyou toilet do it to Cale Barlowe''s' Deluca Hearns Lee Lee upcoming Louis Swanson sixteenth! http://t.co/neZrKLfQ\",\n",
       " \"@DiscoMundial Regretfully, non even. Want to drop-dead to Nicole atomic-number-85 Jerry in Marching-Music, simply couldn't relieve-oneself it. :]\",\n",
       " \"Get-Hold-Of the shift to read Evie and Christopher Shelton-Jackson-Lee''s' Baker Dicaprio al-qur'an. It whitethorn constitute limited Gann simply ANE have-it-away it leadersince it innovate pine-tree-state to his puzzle-out.\",\n",
       " \"Tag help for Martha future Sun, Marcus twentieth at Moomba! #Alessandra&amp;Felicia along stagecoach to do +start a splendid nox! Don't escape information-technology y'all\",\n",
       " \"1 like that @CharlieWaters telecasting http://t.co/FxQYx9hq Peiper Albert Amy Susan hangingwith Lula outside THYMINE.G.I.Friday's\",\n",
       " 'CRIDER to @briannafrost for making it to a THOMPSON Sanders secret-plan tomorrow! Alright fate peeress!',\n",
       " \"I like this @itsAishaAmiri television http://t.co/0WCGjdSN Borsellino Robert Henry Dougie therewith Marcia inch front-man ST.G.I.Friday's (one-hundred-fifty\",\n",
       " \"@YouTube Dude, it be lamentable articulate so-long to this STANLEY last-place nox. I'll forever commend my first Michael Delcid youwhen we stay upward that overnight...\",\n",
       " '@Gonzalexx SUPERB!! An bolshevik simply give-birth year! Milton thatyour sun frolic atomic-number-85 Sunnybank along Day?',\n",
       " 'Bradley Huff be create mosaic for that eighty-sixth t. place of a second ave metro. The make pine-tree-state hatred an undertaking that batch less. Spencer play, MTA.',\n",
       " \"Look fore to go-through Gaines Close''s' instalment atomic-number-85 a eighty-sixth and 2d Sarah railway! Scott Shoemaker @sometimesrhymes theme http://t.co/ax5fOrA8\",\n",
       " '@nyTimes Severely. Endure to Wright cost an pail tilt token of landmines. Also energize for this Parga record-album whatever deteriorate tomorrow.',\n",
       " \"I'll cost drop-a-line inward Lee Crowder for Merrill that Andrea he exist an just penis (sometime) of Segarra and a upright political-leader inch DC\",\n",
       " 'Scorn going forth along Sat, information-technology seem the-like David Miller could equal splendid for May http://t.co/sP54wP5G',\n",
       " '@briannafrost 1 laugh an small, merely non deal them away therewith anybody. Bequeath the conditions live near tomorrow for Bryant Bike?',\n",
       " 'Skilful luck to @LilTunechi tomorrow at an REGISTER Backup!! Do toyour matter brother!! #GoBobcats #MACBaseball',\n",
       " 'Nevertheless a cause to follow to that Stevenson two Lisa Deanne Cynthia atomic-number-85 Sirois Ocho: we bequeath hold an MYTHOLOGIC... http://t.co/LMJiOYeQ',\n",
       " '@ChewB30 ACE believe information-technology be the football-game squad astatine first till IODINE scroll downwards and visit Emma Edmund-Burke. Hernandez',\n",
       " 'An Dominguez was thusly undecomposed final midnight, still my non-use-basketball wish flatmate seat through-with a unharmed everything.',\n",
       " 'ALVIN Bank We receive uyou into #LittleHavana merely delight act non take-down a thirties bldg along an niche of CARL twelfth &amp; Joel Ocho. Retain an window-dressing.',\n",
       " \"Adair this legal-age of my 24-hour-interval ride Geraldine Bicycle, play that demented biz of frizbi and barely worldwide lobstering inward that lord's-day :( #Summer2012\",\n",
       " \"@ShotMyLover SINGLE desire hers thus unhealthy...... ohhh, inch Melodifestivalen uyou think? Yes yep, merely a volition ne'er pass-off. Rameriz constitute fifteenth :']\",\n",
       " 'Keen forward-motion inwards pedagogue trainee atomic-number-85 Bridge-Over that Georgiana the week : advantageously do! See uracil tomorrow astatine One More-Than Breath atomic-number-85 Baldridge Agatha-Christie!',\n",
       " '@annahades This carnage along Espinoza eleven inch Terry al-Zeitoun was like: twenty-five child kill hangingwith scalpels, twenty filles: https://t.co/t6ngSGrr',\n",
       " 'Big Sarah Fuchs be along at 3(EST) tomorrow! give certain you follow an peachy film always :/',\n",
       " 'Choice Mon! Leah upward at-that-place therewith Annette William-Ashley-Sunday. #NCAAbaseball',\n",
       " \"@farkaseyeblack and-so it conk to show that the uyou arrange equal move aboutyour mouthpiece merely uyou can't of-all-time follow to ours face total to allderdice tomorrow beef\",\n",
       " 'Brain of highway Vivian Lynn verbalize @Martynshaw case about drive improvement tomorrow at James Reilly http://t.co/ki5DEXyd',\n",
       " 'Nona crataegus-oxycantha get birth one of a real Lind Perez mixtape, whatsoever.',\n",
       " '@wtju911FM I learn my category to depend in centimes astatine first-gear. ATOMIC-NUMBER-53 english-hawthorn be percentage of an troubles.',\n",
       " \"Consecrate a fact that now personify this eighth of William and that terminated this domain we fete a Mcmillan Girl''s' Friday,... http://t.co/RMyYQyXK\",\n",
       " 'Poole stool. Guild black-market donate the hers wage from Chard to facilitate a Davis Quake. That changes thing.',\n",
       " \"Don't look atomic-number-85 that unsound thing which cost pass-on rear; day be go, nowadays be today and tomorrow be even-so to amount -- Wilkes Nelson\",\n",
       " 'two-thousand-twelve Edwards twenty-nine Frisby Elmer Curtis , Ratitng: Check Information-Technology, That Decent: -It&amp;#39;s Unequaled: Dissimilar eighty-five% of this product atomic-number-49 Genus... http://t.co/RFeXNi3A',\n",
       " \"I've constringe it downward to David and Kimberly ca-ca away along chrome!SetPrinterInfo+0xcfc. Angel whitethorn make-believe this rattling harsh to work-out knocked-out.\",\n",
       " \"ok and-so everybody world-health-organization think information-technology 'InternationalDirectionerDay' Thomas foryour fallacious Directioners Day be twenty-third of july that solar-day they equal constitute! smh...\",\n",
       " 'Ready for this Johnson Mary Anderson Amanda up along Sunnybank. Hamilton 3pm. Cake, quiche &amp; alternative retroactive lunch chow! A unwelcome. http://t.co/H5mkJuFz',\n",
       " \"@AllieBrown7 how's was that string, HUDSON second spear? Raposa medicine, coz information-technology Fuentes Kanno! Patricia Peete medicine creater!\",\n",
       " \"We've land along an humans celebrated William Ocho! Beour modern situation be turn-up along sixteenth Withrow RICKY eighth Ava astatine the... http://t.co/mCCqA1Ti\",\n",
       " \"NASCAR''s' Howell Patrick wins for seventh sentence atomic-number-85 Capital-Of-Delaware: Dr.-Johnson, Dwight and early gang member wear-upon that fruity wigging date... http://t.co/Kg72q3f1\",\n",
       " 'Serve uyou recall Melodifestivalen\\'\\'s\\' \"Mystery-Story\" jest-at @dbaswe? They be plump-for hangingwith moderately sang-froid fresh videotaped \"Intersection\" http://t.co/IvFADx12',\n",
       " \"Day''s' an nifty day to fling along a awing feeding-bottle of Leblanc Topaz application hangingwith ours Maritza N'Tyour Curtis daily specially! Call eight-hundred-sixty-six-970-8267...\",\n",
       " \"I'm at Hyaena''s' Laurie Janet atomic-number-49 Petersen an weekend hangingwith a screaming Palmer Bill-Russell. Vanwinkle illustrates be e'er DISLODGE, therefore uyou hold an rationalise.\",\n",
       " '@Redheadswaggin ANE recall Sean Tilton exist the bang. Atomic-Number-2 crataegus-oxycantha get a power-point almost it be bore. Just the guy-wire be an do-drugs nut imbecile which tilt have an job',\n",
       " \"Sara Chicks''s' Matos what come along eight Ludwig make a external generator divvy-up crown to totally this womanhood along what make Gallic fair-sex thusly proficient\",\n",
       " '@Diarmuidm77lfc4 Patricia for watch #Econokit ; you english-hawthorn care to expend #bizikent hashtag and Angela early topical-anaesthetic Stpierre american-samoa per #bizitalk?',\n",
       " 'scarcely sire nursing-home from Garden Bloomer and Sunnybank. Ferment once-more tomorrow &gt;.&gt;',\n",
       " \"Acura of Donna cost majestic to personify ingest region indiana this VIRGINIA TROY BARRON fundraiser tomorrow astatine Peter Joey''s' inwards... http://t.co/sTTe7fRB\",\n",
       " 'David Muchachas, Maria uyou quick for this night broad of Mode, Keeney and Mantrap. A Peters Kelly Brian Myers and... http://t.co/ceilJnRB',\n",
       " '@SamCraggsCBC nates uyou delight pronounce #happybirthday to maine for tomorrow or RT/come-after maine IODIN sleep-together uyou MARJORIE Helen',\n",
       " 'So progeny therewith Linda get be solved! FINALLY! My eBook \"Bridge The Variance: Comprehend Pup People Now\" leave cost emitted TOMORROW!',\n",
       " \"Dear Rose Bicyclist, IODIN volition non subscribe thatyour cause ifs uyou be put n'tyour sweaty minute along Preston Wheel! http://t.co/Cn9sADW9\",\n",
       " \"@HStringfellow Abdullah this @DerrickWalton10 #Firesuit commercial-grade liaisonduring a geological-fault, couldn't stop laughs. Regard it first atomic-number-85 that Antwan displaying atomic-number-85 LVMS.\",\n",
       " 'Maria secernate Andrew-Johnson, \"Dear newsletters is we\\'re but twenty-seventh\" whereabouts he start just they be fourth itbefore oftheir difficulties. JJ tell plump to the face',\n",
       " \"Carnival Tomorrow. I'll make-up certain to hire word-picture:) #Excited\",\n",
       " 'It\\'\\'s\\' MAKE-INFORMATION-TECHNOLOGY-HAPPEN Monday!Git \\'atomic-number-68 do. \"To h*ll therewith CIRCUMSTANCE; I make my possess SHOTS.\"-Bruce Robert-E.-Lee, star &amp; courts-martial \\'art caption',\n",
       " \"Mcbride twenty-third dame detached trough eleven!! Lopez all uyou postulate to make-out! Don't escape an ace! Gary brandish vol1 bequeath exist previ http://t.co/MxW6uau2\",\n",
       " 'Tues, Null twelve, is a Republican Jeff inwards Old-Dominion-State. IODIN firmly commend Delisio Robin for UNITED-STATES Lawmaker! http://t.co/9lgkQ2lx',\n",
       " '@AmyAnneTurner Walker for accompany #Econokit ; uyou crataegus-oxycantha wish to employ #bizikent hashtag and Edward alternative local-anaesthetic clientele antiophthalmic-factor per #bizitalk',\n",
       " '@dbaswe? Take the super excursions. Get-A-Line you tomorrow atomic-number-85 an Tracy Auberge?',\n",
       " \"I'm wearing my Kony two-thousand-twelve jersey for an first time-limits. 1 palpate feeble lmao :]\",\n",
       " \"@KeiJhoHyun28 thus we arrive to visualise it in Toevs tomorrow on RODRIGUEZ Communiques. Can't awaits. -Congratulations along an minuscule roll indium this kiln #amazeballs tenner\",\n",
       " 'Alice what @TeachersFed scarcely perpetrate upwardly to my household shell Ward Kevin Mason Tomorrow. Know an men.',\n",
       " '@wtju911FM Kimberly and Upasana father tie to that fresh Boyden of spirit and may Deity lay-down that Trainor substantial and strong clarence-day away 24-hour-interval',\n",
       " \"Dann Gets this Greenish Walker Heath Robert Telecommunication: PEKING, Devin fourteen, two-thousand-twelve /PRNewswire/ --Communist-China Telecom''s' Guan... http://t.co/H8hx6o2b\",\n",
       " \"Listen to Edgar Charlie-Parker''s' Misty King mixture along Spotify. 1 call-back that reality english-hawthorn wee me the painful prisoner.\",\n",
       " \"It''s' risible how's I'm the-like this alone eighth grader ICONiac....\",\n",
       " 'Know hang away therewith that mass astatine Unseen Laine along monday! So awing and inspire! :/ @dbaswe? http://t.co/AoaEsDAU',\n",
       " 'We embody transport to feature bene postulate to roleplay hangingwith a Shirker indium Whelans along Juana seventh...a splendid way-of-life to lash-out away that Moffett weeks!!!!!',\n",
       " '@ReturnOfTheMac6 ifthe uyou call-for that raw unrivalled english-hawthorn IODIN advise \"Calle Ocho\" installation... \"Iinternational Love\" place... \"Move Shingle Throw\"... etcetera #dale',\n",
       " '@nyTimes @guardiannews Cathy be my Bday calendar-month also! Josefa Cecelia ought set information-technology thusly ONE buns get the Andrew Embracing! RYAN',\n",
       " \"@YouTube ANE roll-in-the-hay!! 1 posture downwards I'm exactly same OHMYGOSH that''s' Douglas Newby\",\n",
       " \"@briannafrost I fuck I'm tardy, simply I savour encounter uyou atomic-number-85 Tina indiana March! I make-up from Bethlehem-Ephrathah, SILVER Kendall uyou along Get-Up. :-}\",\n",
       " '@Kent_BA Sandra for come-after #Econokit ; uyou crataegus-laevigata care to utilization #bizikent and Angela different local occupation a per #bizitalk',\n",
       " '@richardm56 On a agency to Cornwall hearing to uyou. By this path information-technology live Father Carol unofficially. Eff a euphony therefore interahamwe',\n",
       " \"@stevebartram1 Yeah, only Women' Garrison coming from Generate Day, doesn't it? As Father-God' Timothy be precisely an standing-operating-procedure to terminate them quetch.\",\n",
       " 'A subject world-health-organization uyou follow, nigh of this chic mass crop for mortal else. ~ Robert Glee, corp-founder of Mason Microelectronics. #inverted-comma',\n",
       " 'Jack honour win away ours fantastic event squad ; ours year-on-year league embody range good connexion league in the GREAT-BRITAIN! http://t.co/6caEND86',\n",
       " 'Get y-your growler from @gealingeman and fulfil it up hangingwith centime past James twenty-two! http://t.co/OdURubHY #yyc #foundation',\n",
       " '@AmyAnneTurner my design for upcoming Roberts be personify along site inwards adjacent JACKELYN flick wish-well maine felicitous b clarence-day nrc-bri nrc-bri',\n",
       " '- Shinji Alfred entertain the coach polyclinic for scholar on Wednesday, to bear-out fry atomic-number-49 sphere strike aside an Marina Temblor.',\n",
       " \"IODIN can't waitress to witness altogether this riot fight indiana Gerald along Brown tomorrow\",\n",
       " 'Emily James-Augustus-Henry-Murray\\'\\'s\\' @CarlJWood \"Understandwhat Sheri Tony-Blair doesn\\'t infer well-nigh toothsome mamma\" http://t.co/eVYwYVfm',\n",
       " \"Three of beour Helen squad embody away to James hangingwith Blalock Valle tomorrow axerophthol they're make for an Schuette Amy for adept residential-district newsprint\",\n",
       " 'Assad continue to bomb #Homs, four that seventeenth back-to-back day-care, roving ntwrks away four a quaternary twenty-four-hour-period, mjr engagement about Janice Rosa #Syrian-Arab-Republic',\n",
       " '@espn youwhen mccann get beour tertiary finish iyan Hyatt become and winkle to united-states-of-america swish supporters, he roll-in-the-hay it #trueposh',\n",
       " 'I bonk Verna that suffer upwardly for an Wright and that Edwards relatives. We crataegus-laevigata non stimulate this gravid fan foundation, just we sure-enough dress bear a solid.',\n",
       " \"Why wouldn't Timothy Hernandez commence this eighth hangingwith this large left-handed-pitcher batting-order? Debra an common-sense. Show with that homing-pigeon. #mets\",\n",
       " '@jimdoyle Susan to see-to-it uyou stick-out Braggs along Sabbatum. Mark',\n",
       " 'Leslie Paul Merchandiser, to cite Dunsford Garret, crataegus-laevigata ONE but enunciate, \"foryour Never-Never-Land be juste more-or-less to dip...\".. Karma biatches!! Suck-Up my balls!!',\n",
       " '@hamilton_elle cut aside Mary Tangent ifthe it as-well blistering for a Bettes sunshine.',\n",
       " 'Fashion Crabtree appear prissy later watch-out an first sequence :(',\n",
       " \"@briannafrost fourth embody simply groovy bc it''s' corresponding Cheryl Stephan but ROGELIO #canttouchthis\",\n",
       " '#InMiddeSchool IODIN gravel my first cellular-telephone speech-sound. I be besides merry I receive that telephones. information-technology be an Harriss summerset telephone lol ah that equal that sidereal-day lol.',\n",
       " '@DJPaulyD Braggs MCBRIEN Daniel Johnnie get-ahead a \"Indie\" categories of the seventeenth Yearbook Drummies! http://t.co/l64Xc2Lq',\n",
       " 'Lawson Calderon wins a \"Indie\" categories of the seventeenth Annual Drummies! , http://t.co/aiuTIgbj via @OhHeyAl',\n",
       " '@meow_x Case ONE atomic-number-53 like that information-technology follow Magana already thusly that I throne decease to Bessie Jamming. They cause yet to foretell an bank-line up. :FIVE-HUNDRED',\n",
       " 'Ohio 1 wonder ifs @mscutiepie143 exist make-out this john-wilkes-booth inward Terry for that fourth of Months! Gotta have certain peuples be-intimate a MILLER of POUTINE! lol YUMMO',\n",
       " 'My brother @sheep1903 wasgonna deal a reign from troika burke without that dropoff and live along an stage indiana an few twelvemonth. gta gt tht teaching first!',\n",
       " 'Musical awareness-building: Turner Dominique Arlene Eric give an stop, Now constitute a clip do non',\n",
       " '@annahades first oppugn: Later-On run hangingwith the jetty combat-injury at that fuse, whats be Guys functionary forty metre at his Scott Day? four.53?',\n",
       " 'A final two year suffice Crystal atomic-number-85 DAVID along Sanchez night get teach pine-tree-state to-a-greater-extent approximately the substance of that religious-doctrine than SINGLE teach inward a thirty-two year b4',\n",
       " \"Heading to this Gutierrez Stimpert Masri outcome astatine Brady Marie Casino''s' Joseph tomorrow. Fahey shake-up! #DET\",\n",
       " 'Just drive done Stalcup Christopher along the Friday knight. Tavano know information-technology be Sturgill brickell. External legal-community, tarry, taphouse... Matter-To..',\n",
       " \"#BestMixtapesEver I wasn't this @Carolyn1202a buff till UNITY learn that first Richard Gangsta Grillz. Bettina my overall impression.\",\n",
       " '@richardm56 1 reckon a personify hardly Marshall Rickman? Aucoin Giraldo (beaver-state Clark Lupine, if uyou crataegus-oxycantha) be mined! We already tally along that!',\n",
       " \"information-technology''s' buy-the-farm downwardly Finneran sixth @DannycatedLover #leeds calle Jeffers summertime political-party that Larose right #salsa party. Not to follow overlook http://t.co/B7Aqjdgx\",\n",
       " \"waiting, don't narrate me he rank seventh? angstrom-unit Fashion King, truly?\",\n",
       " \"On Juan fourteenth, Ten Thousand Mincy confirm @Martynshaw http://t.co/X9NxxFX1 Attend how's uyou toilet service.\",\n",
       " '@MTV Ann to that Realm, simply the be workaday. Today atomic-number-49 unique...Great Big Allen Tomorrow.',\n",
       " '@rossmbrown Nina, Warner uyou parcel hangingwith readers, Last Year inward a Ramsey of Clear Marilyn-Monroe, Bond be detached now and Friday! http://t.co/m1eLqi5M',\n",
       " \"Ifthe uyou roll-in-the-hay life-sentence, don't waste bedtime, for clip comprise what's life-time be make upwards of.-David-Bruce Tsung-Dao-Lee. Set-About a PAUL come-out of an Fri! #leapinno\",\n",
       " '@REAMER_72 @Martynshaw THOMPSON crataegus-laevigata equally well Ester Juncaj information-technology a 24-hour-interval trough Rosiness. Whole clarence-day in at-that-place take-care fairly nice. Start hangingwith Mary Dreams.....',\n",
       " 'Turn away Yoko Puccia follow this gross track to this showery Tara first-light.',\n",
       " '1 crataegus-laevigata be deliberate wear my polka department-of-transportation #forty-eight underdrawers therewith that Halcomb teeing-ground clothing indiana this endeavour to outride cooling present-day. #Lowes48',\n",
       " \"@Kdizzle3434 Huberty Conan! Smith serve that doorway!! Information-Technology''s' Mary Janice Romney''s' SIXTH boys, and he wants adequate timeframes!!!\",\n",
       " '[trans]TV snap calendar-week start. first film this-night LENIGAN NANCY DIANE hold-out broadcast. WOODS leave usher u this new piece! (antiophthalmic-factor ,.. http://t.co/SSNvRmk0',\n",
       " \"Thanks to Moomba, I've be cruise wisconsin this Roger leadersince Davis two-thousand-eleven. Today ONE adjudicate to in-the-end become pose information-technology do! #abouttime\",\n",
       " \"information-technology''s' tues :1D #neptunium moanin : yuko kanno (ost. sakamichi a apollon)\",\n",
       " 'want uyou contented Martell , eight Borderland: Foster Hughes Storm Dining-Compartment At ?..: Information nigh Kelly &amp; Tec... http://t.co/vJPF4hy3 #asematy',\n",
       " \"Baldridge derive present-day! Tomorrow embody correct information-technology upward and examine if information-technology an work clarence-day lol. Ifs that kick-the-bucket a design I'll comprise stream twenty-four-7 before-long!\",\n",
       " 'One concern approximately Richard make-up that it be buy aside Harper lately and hence crataegus-laevigata no-more long sustain identical sustenance/development a formerly.',\n",
       " \"Uyou ne'er forget ofyour beginning lovers, non surprisec indiana King-Oedipus''s' caseful helium ensure hers this Beget Lord'S-Day.\",\n",
       " 'http://t.co/KV0EiWwP, Alexander uniting english-hawthorn make-up indium danger, report-card says, Ruff eminence: See this residue of an storey atomic-number-2... http://t.co/KPJIXqwl',\n",
       " \"Information-Technology''s' Espouse Wed! Today along an web-log we be share this beautiful Greve West wedding. http://t.co/1urWNUKd\",\n",
       " 'Syria,Homs,Baba Debra nether this blast great bombardment away Petrik assad and his hoodlum 22 february two-thousand-twelve http://t.co/HwykUppP',\n",
       " '@EmRusciano Remember think it be significative inwards Burns two-thousand-three: magnificent v Chisholm midweek, struggle vs Lori along Sabbatum. Margaret actor tho',\n",
       " 'Happy thirteenth Bday to my nephew Joe Yocum TWO! Janet sleep-together uyou Grandpa',\n",
       " 'Unity Shouse leave legion that Gill George Movie along Sparks :}',\n",
       " 'Ifthe ur proceed two #SDCC, Bellamy first aspect at Ryan Easton\\'\\'s\\' new vivid fresh \"Credenza,\" cubicle ordinal four-thousand-four-hundred-one. Exciting news-program!',\n",
       " 'Baseball-Game biz hangingwith Sophia tomorrow #goddoesgoodthing @iansomerhalder we r. Back @moanajkidd @DiscoMundial',\n",
       " \"@DannycatedLover Can't levitate o'er join to run-across charge public-figure? Andre doesn't stimulate playlist and an unique mortal simply deliver an first sequence in-any-event.\",\n",
       " \"@itsAishaAmiri draw-a-blank flogging indium that dominicus. Dardis to Czarnota Tan and develop n'tyour tan along in just-now minute... Recoverer, Powell and Privately!\",\n",
       " 'Concern: More Tudou stave whitethorn keep-an-eye-on headman out that trapdoor. http://t.co/jfWk5FeP',\n",
       " 'Very much look ahead to a @ShotMyLover @ChewB30 tomorrow hangingwith Dawson Fancy, @moanajkidd &amp; @sometimesrhymes ; chika chika ye.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['random_synonym_95']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Watched a movie yesterday #The70's on #OVTV and was 'pleasantly' surprised-- to see Michael Easton (John, #OLTL) in it.\"\n",
    "tokens = text.split()\n",
    "nltk_tokens = nltk.word_tokenize(text)\n",
    "pos_tags = [val[1] for val in nltk.pos_tag(nltk_tokens)]\n",
    "orig_to_pos_mapping, _ = match_pos_token_to_original(nltk_tokens, tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pos_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39menumerate\u001b[39m(pos_tags))\n\u001b[0;32m----> 2\u001b[0m orig_to_pos_mapping\u001b[39m.\u001b[39;49mmap(pos_dict)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "pos_dict = dict(enumerate(pos_tags))\n",
    "orig_to_pos_mapping.map(pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_to_drop = [\"$\", '', \"(\", \")\", \",\", \"#\", \"POS\", \"--\", \".\", \":\", \"''\"]\n",
    "\n",
    "new_pos_tags = []\n",
    "for k in orig_to_pos_mapping.keys():\n",
    "    if len(orig_to_pos_mapping[k]) == 1:\n",
    "        new_pos_tags.append(pos_tags[orig_to_pos_mapping[k][0]])\n",
    "    else:\n",
    "        to_add = []\n",
    "        for i in orig_to_pos_mapping[k]:\n",
    "            if pos_tags[i] not in pos_to_drop:\n",
    "                to_add.append(pos_tags[i])\n",
    "        if len(to_add) == 1:\n",
    "            new_pos_tags.append(to_add[0])\n",
    "        else:\n",
    "            print(to_add)\n",
    "            new_pos_tags.append('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VBD',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'CC',\n",
       " 'VBD',\n",
       " 'RB',\n",
       " 'JJ',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'PRP']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'./Tokenizers/Pretrained/{model_name}.pt', ## Saved locally\n",
    "                                            do_lower_case = False ## Check if BERT is uncased\n",
    "                                            )\n",
    "\n",
    "\n",
    "collator = ClassificationCollator(tokenizer=tokenizer,\n",
    "                                    text_label=text_label,\n",
    "                                    idx_label=idx_label,\n",
    "                                    test_only=True,\n",
    "                                    max_seq_len=MAX_LENGTH)\n",
    "\n",
    "if experiment_time == True:\n",
    "    g = { 'TOKEN': tokenizer.unk_token }\n",
    "    test_dataset = load_from_disk(f\"./Data/Noise/{test_data}\")['test'].format(**g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data(model_name, data_name, device, max_size=None, sequential=False, experiment_time = False):\n",
    "    \n",
    "    ####################### Determine Tokenizer rules #############################\n",
    "    \n",
    "        \n",
    "    if data_name == 'SST-2':\n",
    "        MAX_LENGTH = 64\n",
    "        is_lower = True\n",
    "    else:\n",
    "        MAX_LENGTH = 256\n",
    "        is_lower = False\n",
    "        \n",
    "    idx_label = \"index\"\n",
    "    text_label = \"text\"\n",
    "    test_data = data_name\n",
    "\n",
    "        \n",
    "    if \"electra\" in model_name: ## Must be lowercase\n",
    "        is_lower = True\n",
    "        \n",
    "        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(f'./Tokenizers/Pretrained/{model_name}.pt', ## Saved locally\n",
    "                                                do_lower_case = is_lower ## Check if BERT is uncased\n",
    "                                                )\n",
    "    \n",
    "    if 'gpt2' in model_name:\n",
    "        # default to left padding\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        # Define PAD Token = EOS Token = 50256\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    collator = ClassificationCollator(tokenizer=tokenizer,\n",
    "                                      text_label=text_label,\n",
    "                                      idx_label=idx_label,\n",
    "                                      test_only=True,\n",
    "                                      max_seq_len=MAX_LENGTH)\n",
    "    \n",
    "    if experiment_time == True:\n",
    "        g = { 'TOKEN': tokenizer.unk_token }\n",
    "        test_dataset = load_from_disk(f\"./Data/Noise/{test_data}\")['test'].format(**g)\n",
    "\n",
    "    else:\n",
    "\n",
    "        ##### Get datasets and tokenize them\n",
    "        if max_size:\n",
    "            test_dataset = load_from_disk(f\"./Data/Clean/{test_data}\")['test'][:max_size]\n",
    "            test_dataset = Dataset.from_dict(test_dataset)\n",
    "        else:\n",
    "            test_dataset = load_from_disk(f\"./Data/Clean/{test_data}\")['test']  \n",
    "\n",
    "    print(\"Dataset size: \", test_dataset.num_rows)\n",
    "\n",
    "    # Create the DataLoaders for our training and validation sets.\n",
    "    # We'll take training samples in random order. \n",
    "    test_dataloader = DataLoader(\n",
    "                test_dataset,  # The training samples.\n",
    "                sampler = SequentialSampler(test_dataset) if sequential else RandomSampler(test_dataset), # Select batches randomly\n",
    "                batch_size = 1, # Test with this batch size.\n",
    "                collate_fn=collator\n",
    "            )\n",
    "    \n",
    "    print(\"Loaded data: \", data_name)\n",
    "\n",
    "    ########################### LOAD MODEL ####################################################\n",
    "    \n",
    "    if model_name.split('-')[0] == 'bert':\n",
    "        \n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            f'./Models/Pretrained/{model_name}.pt', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                            # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "        model.load_state_dict(torch.load(f'./Models/{data_name}/bert.pt', map_location=device))\n",
    "    elif model_name.split('-')[0] == 'roberta':\n",
    "        model = RobertaForSequenceClassification.from_pretrained(\n",
    "            f'./Models/Pretrained/{model_name}.pt', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                            # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "        model.load_state_dict(torch.load(f'./Models/{data_name}/roberta.pt', map_location=device))    \n",
    "    elif model_name.split('-')[0] == 'electra':\n",
    "        model = ElectraForSequenceClassification.from_pretrained(\n",
    "            f'./Models/Pretrained/{model_name}.pt', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                            # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "        model.load_state_dict(torch.load(f'./Models/{data_name}/electra.pt', map_location=device))    \n",
    "    elif 'gpt2' in model_name.split('-')[0]:\n",
    "        \n",
    "        model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=f\"gpt2-medium\", num_labels=2)\n",
    "        model = GPT2ForSequenceClassification.from_pretrained(\n",
    "            f'./Models/Pretrained/{model_name}.pt', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            config = model_config\n",
    "        )\n",
    "        # resize model embedding to match new tokenizer\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        # fix model padding token id\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "        \n",
    "    else:\n",
    "        print(\"unaccepted model type. Received: \", model_name.split('-')[0])\n",
    "        return -1\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    print(\"Loaded model: \", model_name)\n",
    "    return test_dataloader, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'lemma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nltk_tokens[\u001b[39m5\u001b[39;49m]\u001b[39m.\u001b[39;49mlemma()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'lemma'"
     ]
    }
   ],
   "source": [
    "nltk_tokens[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sara', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('boyfriend', 'NN'),\n",
       " ('had', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('lavish', 'JJ'),\n",
       " ('berry', 'NN'),\n",
       " ('birthday', 'NN'),\n",
       " ('party', 'NN'),\n",
       " ('last', 'JJ'),\n",
       " ('evening', 'NN')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sara',\n",
       " \"'s\",\n",
       " 'boyfriend',\n",
       " 'had',\n",
       " 'a',\n",
       " 'lavish',\n",
       " 'berry',\n",
       " 'birthday',\n",
       " 'party',\n",
       " 'last',\n",
       " 'evening']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_1806869/1313937350.py\u001b[0m(88)\u001b[0;36mcreate_random_masks\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     86 \u001b[0;31m        \u001b[0mnoisy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'random_token_{prop*100:2.0f}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{TOKEN}'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     87 \u001b[0;31m        \u001b[0mnoisy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'random_charswap_{prop*100:2.0f}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharswap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 88 \u001b[0;31m        \u001b[0mnoisy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'random_synonym_{prop*100:2.0f}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordswap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     89 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     90 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mnoisy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "0.95\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "['I', 'had', 'a', 'lavish', 'berry', 'birthday', 'party', 'last', 'evening']\n",
      "['I', 'had', 'a', 'lavish', 'berry', 'birthday', 'party', 'last', 'evening']\n",
      "0.05\n",
      "*** AttributeError: 'numpy.ndarray' object has no attribute 'wordswap'\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper_py3-8/lib/python3.8/site-packages/nltk/corpus/reader/api.py:219\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen(f) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m--> 219\u001b[0m         contents\u001b[39m.\u001b[39mappend(fp\u001b[39m.\u001b[39;49mread())\n\u001b[1;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper_py3-8/lib/python3.8/site-packages/nltk/data.py:1055\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \u001b[39mRead up to ``size`` bytes, decode them using this reader's\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39mencoding, and return the resulting unicode string.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[39m:rtype: unicode\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1055\u001b[0m chars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read(size)\n\u001b[1;32m   1057\u001b[0m \u001b[39m# If linebuffer is not empty, then include it in the result\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper_py3-8/lib/python3.8/site-packages/nltk/data.py:1338\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader._read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[39mif\u001b[39;00m size \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1338\u001b[0m     new_bytes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m   1339\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mMemoryError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreader\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwordlist\u001b[39;00m \u001b[39mimport\u001b[39;00m MWAPPDBCorpusReader\n\u001b[1;32m      3\u001b[0m ppdb \u001b[39m=\u001b[39m MWAPPDBCorpusReader(root\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/home/fvd442/project/noise-paper/\u001b[39m\u001b[39m'\u001b[39m, fileids\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mppdb-2.0-xxxl-lexical\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m ppdb_synonyms \u001b[39m=\u001b[39m ppdb\u001b[39m.\u001b[39;49mwords()\n\u001b[1;32m      7\u001b[0m clean_ppdb_synonyms \u001b[39m=\u001b[39m {}\n\u001b[1;32m      9\u001b[0m redo_lines \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper_py3-8/lib/python3.8/site-packages/nltk/corpus/reader/wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords\u001b[39m(\u001b[39mself\u001b[39m, fileids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ignore_lines_startswith\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m     20\u001b[0m         line\n\u001b[0;32m---> 21\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m line_tokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw(fileids))\n\u001b[1;32m     22\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[1;32m     23\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/noise-paper_py3-8/lib/python3.8/site-packages/nltk/corpus/reader/api.py:219\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fileids:\n\u001b[1;32m    218\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen(f) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m--> 219\u001b[0m         contents\u001b[39m.\u001b[39mappend(fp\u001b[39m.\u001b[39;49mread())\n\u001b[1;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m concat(contents)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordlist import MWAPPDBCorpusReader\n",
    "\n",
    "ppdb = MWAPPDBCorpusReader(root='/home/fvd442/project/noise-paper/', fileids='ppdb-2.0-xxxl-lexical')\n",
    "\n",
    "ppdb_synonyms = ppdb.words()\n",
    "\n",
    "clean_ppdb_synonyms = {}\n",
    "\n",
    "redo_lines = []\n",
    "for line in ppdb_synonyms:\n",
    "    try:\n",
    "        l = line.split(' ||| ')\n",
    "        if l[1].startswith('<'):\n",
    "            continue\n",
    "        if l[5] == 'Equivalence':\n",
    "            l[0] = l[0][1:-1]\n",
    "            if l[1] in clean_ppdb_synonyms.keys():\n",
    "                if l[0] in clean_ppdb_synonyms[l[1]].keys():\n",
    "                    clean_ppdb_synonyms[l[1]][l[0]].append(l[2])\n",
    "                else:\n",
    "                    clean_ppdb_synonyms[l[1]][l[0]] = [l[2]]\n",
    "            else:\n",
    "                clean_ppdb_synonyms[l[1]] = {l[0] : [l[2]]}\n",
    "    except IndexError:\n",
    "        redo_lines.append(line) ### ends up being irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"ppdb_synonyms.json\", \"w\") as outfile:\n",
    "    # json_data refers to the above JSON\n",
    "    json.dump(clean_ppdb_synonyms, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus.reader.wordlist import MWAPPDBCorpusReader\n",
    "\n",
    "# ppdb = MWAPPDBCorpusReader(root='/home/fvd442/project/noise-paper/', fileids='ppdb-2.0-xxxl-lexical')\n",
    "\n",
    "# ppdb_synonyms = ppdb.words()\n",
    "\n",
    "clean_ppdb_synonyms_XL = {}\n",
    "\n",
    "redo_lines = []\n",
    "for line in ppdb_synonyms:\n",
    "    try:\n",
    "        l = line.split(' ||| ')\n",
    "        if l[1].startswith('<'):\n",
    "            continue\n",
    "        if l[5] in ['Equivalence', 'ForwardEntailment', 'ReverseEntailment']:\n",
    "            l[0] = l[0][1:-1]\n",
    "            if l[1] in clean_ppdb_synonyms_XL.keys():\n",
    "                if l[0] in clean_ppdb_synonyms_XL[l[1]].keys():\n",
    "                    clean_ppdb_synonyms_XL[l[1]][l[0]].append(l[2])\n",
    "                else:\n",
    "                    clean_ppdb_synonyms_XL[l[1]][l[0]] = [l[2]]\n",
    "            else:\n",
    "                clean_ppdb_synonyms_XL[l[1]] = {l[0] : [l[2]]}\n",
    "    except IndexError:\n",
    "        redo_lines.append(line) ### ends up being irrelevant\n",
    "        \n",
    "import json\n",
    "with open(\"ppdb_synonyms_xxxl.json\", \"w\") as outfile:\n",
    "    # json_data refers to the above JSON\n",
    "    json.dump(clean_ppdb_synonyms_XL, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ppdb_synonyms_XXL = {}\n",
    "\n",
    "redo_lines = []\n",
    "for line in ppdb_synonyms:\n",
    "    try:\n",
    "        l = line.split(' ||| ')\n",
    "        if l[1].startswith('<'):\n",
    "            continue\n",
    "        if l[5] in ['Equivalence', 'ForwardEntailment', 'ReverseEntailment']:\n",
    "            if l[1] in clean_ppdb_synonyms_XXL.keys():\n",
    "                clean_ppdb_synonyms_XXL[l[1]].add(l[2])\n",
    "            else:\n",
    "                clean_ppdb_synonyms_XXL[l[1]] = set([l[2]])\n",
    "    except IndexError:\n",
    "        redo_lines.append(line) ### ends up being irrelevant\n",
    "        \n",
    "for k in clean_ppdb_synonyms_XXL.keys():\n",
    "    clean_ppdb_synonyms_XXL[k] = list(clean_ppdb_synonyms_XXL[k])\n",
    "\n",
    "with open(\"ppdb_synonyms_xxxl_nopostag.json\", \"w\") as outfile:\n",
    "    # json_data refers to the above JSON\n",
    "    json.dump(clean_ppdb_synonyms_XXL, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noise-paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
