{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fvd442/anaconda3/envs/noise-paper/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from perturbation_utils import *\n",
    "from datasets import Dataset, load_from_disk, DatasetDict\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'bert'\n",
    "DATA = 'SST-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_path_dataset = f\"./Data/Clean/{DATA}\"\n",
    "\n",
    "\n",
    "dataset = load_from_disk(from_path_dataset)['test'][:50] #['test']\n",
    "dataset = Dataset.from_dict(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_human_masks(example, anno):\n",
    "    global pos_tags\n",
    "    noisy = {}\n",
    "    ranked_anno = np.argsort(anno)\n",
    "    \n",
    "    strategic_anno = [] # Hold strategic annotations\n",
    "    for i, v in enumerate(anno):\n",
    "        if v != 0.0:\n",
    "            strategic_anno.append(v)\n",
    "        elif pos_tags[i].startswith('J'): # ADJECTIVE\n",
    "            strategic_anno.append(0.3) ### Minimum value in list is 0.333 so this will be after the list\n",
    "        elif pos_tags[i].startswith('R'): # ADVERB\n",
    "            strategic_anno.append(0.25)\n",
    "        elif pos_tags[i].startswith('V'): # VERB\n",
    "            strategic_anno.append(0.2)\n",
    "        elif pos_tags[i].startswith('N'): # NOUN\n",
    "            strategic_anno.append(0.1)\n",
    "        else:\n",
    "            strategic_anno.append(0.05)\n",
    "            \n",
    "    ranked_strat_anno = np.argsort(strategic_anno)\n",
    "    \n",
    "    \n",
    "    ##### Note when human annotation ends\n",
    "    mask = (anno != 0)\n",
    "    noisy[f'human-A_token'] = ' '.join(np.where(mask, '{TOKEN}', example))  \n",
    "    noisy[f'human-A_charswap'] = ' '.join(np.where(mask, random_charswap(example), example)) \n",
    "    noisy[f'human-A_synonym'] = ' '.join(np.where(mask, wordswap(example), example)) \n",
    "\n",
    "    for prop in [0.25,0.5,0.7,0.8,0.9,0.95]:\n",
    "        \n",
    "        ## Random fill\n",
    "        mask = [True if ele in ranked_anno[-round(len(anno)*prop):] else False for ele in np.arange(len(anno))]\n",
    "        noisy[f'human-R_token_{prop*100:2.0f}'] = ' '.join(np.where(mask, '{TOKEN}', example))  \n",
    "        noisy[f'human-R_charswap_{prop*100:2.0f}'] = ' '.join(np.where(mask, random_charswap(example), example))   #' '.join(np.where(mask,  , example))     \n",
    "        noisy[f'human-R_synonym_{prop*100:2.0f}'] = ' '.join(np.where(mask, wordswap(example), example))   #' '.join(np.where(mask, wordswap(example) , example))  \n",
    "        \n",
    "        ## Strategic Fill  \n",
    "        \n",
    "        mask = [True if ele in ranked_strat_anno[-round(len(anno)*prop):] else False for ele in np.arange(len(anno))]\n",
    "        noisy[f'human-S_token_{prop*100:2.0f}'] = ' '.join(np.where(mask, '{TOKEN}', example))  \n",
    "        noisy[f'human-S_charswap_{prop*100:2.0f}'] = ' '.join(np.where(mask, random_charswap(example), example))   #' '.join(np.where(mask,  , example))     \n",
    "        noisy[f'human-S_synonym_{prop*100:2.0f}'] = ' '.join(np.where(mask, wordswap(example), example))   #' '.join(np.where(mask, wordswap(example) , example))  \n",
    "    \n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_human_noise(example):\n",
    "    global pos_tags\n",
    "    nltk_tokens = nltk.word_tokenize(example['text'])\n",
    "    pos_tags = [val[1] for val in nltk.pos_tag(nltk_tokens)]\n",
    "    pos = {'pos_tags' : pos_tags}\n",
    "    \n",
    "    tokens = example['text'].split()\n",
    "    anno = np.abs(np.array(example['annotations'].split()).astype(float)) ## Chose + or - to indicate if positive or negative word in SemEval\n",
    "    \n",
    "    noise = create_human_masks(tokens, anno)\n",
    "    return example | noise | pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordswap(iterable):\n",
    "    global pos_tags\n",
    "    global NO_MATCH\n",
    "    global no_match_counter\n",
    "    global word_counter\n",
    "    out = []\n",
    "    for i,x in enumerate(iterable):\n",
    "        word_counter += 1\n",
    "        new = find_replacement(x, pos_tags[i])\n",
    "        if new == x:\n",
    "            NO_MATCH.add((x, pos_tags[i]))\n",
    "            no_match_counter += 1\n",
    "        out.append(new)\n",
    "                \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function insert_human_noise at 0x7f489022a9d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "                                                           \r"
     ]
    }
   ],
   "source": [
    "new_dataset = dataset.map(insert_human_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'WDT',\n",
       " 'VBZ',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[0]['pos_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noise-paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
